{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "from Tensorized_components.patch_embedding  import Patch_Embedding     \n",
    "from Tensorized_components.w_msa_w_o_b_sign  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b_sign import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL_CHANGED import TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Utilities for DropPath (Stochastic Depth)\n",
    "# --------------------------------------------------------------------------------\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (5D or 6D for your tensor shape).\n",
    "    This function is generalized for your input shape. Adjust if needed.\"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    \n",
    "    keep_prob = 1.0 - drop_prob\n",
    "    batch_size = x.shape[0]\n",
    "    # For a 6D input, create a mask of shape (B, 1, 1, 1, 1, 1)\n",
    "    random_tensor = keep_prob + torch.rand(\n",
    "        (batch_size, ) + (1,) * (x.dim() - 1),\n",
    "        dtype=x.dtype, device=x.device\n",
    "    )\n",
    "    random_tensor.floor_()\n",
    "    x = x / keep_prob * random_tensor\n",
    "    return x\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Wrapper module for drop_path function.\"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl, embed_shape, dropout=0.0, drop_path_rate=0.0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl = tcl\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1) Window MSA + residual\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (2) TCL + residual\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (3) Shifted Window MSA + residual\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (4) TCL + residual\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl, embed_shape=(4, 4, 6), dropout=0.0, drop_path_rate=0.0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl = tcl\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ----- Window MSA -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ----- TCL -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ----- Shifted Window MSA -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ----- TCL -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinBlock3(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl, embed_shape=(4,4,12),\n",
    "                 dropout=0.0, drop_path_rate=0.0):\n",
    "        super(SwinBlock3, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl = tcl\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Window MSA + Residual\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # 2) TCL + Residual\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # 3) Shifted Window MSA + Residual\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # 4) TCL + Residual\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinBlock4(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl, embed_shape=(4,4,24),\n",
    "                 dropout=0.0, drop_path_rate=0.0):\n",
    "        super(SwinBlock4, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl = tcl\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---- Window MSA ----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ---- TCL ----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ---- Shifted Window MSA ----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # ---- TCL ----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drop_path_rates(num_blocks, max_drop=0.5):\n",
    "    \"\"\"\n",
    "    Returns a list of drop path rates linearly increasing from 0 to max_drop,\n",
    "    length = num_blocks.\n",
    "    Example: if num_blocks=24 and max_drop=0.5, returns [0.0, 0.0217..., ..., 0.5].\n",
    "    \"\"\"\n",
    "    return torch.linspace(0, max_drop, num_blocks).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=384,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 max_drop_path=0.2,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # (Optional) You can define the 'depths' of each stage. \n",
    "        # For instance, [2,2,18,2] for 'Swin-Large' style structure, \n",
    "        # but it can vary.\n",
    "        self.depths = [2, 2, 18, 2]  # matches your example\n",
    "        total_blocks = sum(self.depths)  # e.g. 24 in your case\n",
    "\n",
    "        # Create a list of linearly spaced drop path rates from 0 to max_drop_path\n",
    "        drop_path_rates = create_drop_path_rates(total_blocks, max_drop_path)\n",
    "\n",
    "        # We'll keep track of an index so we can assign a unique drop path rate to each block\n",
    "        dpr_index = 0\n",
    "\n",
    "        # ------------------ Patch Embedding ------------------\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_shape=embed_shape,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "        # ------------------ Stage 1 Setup -------------------\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_1 = TCL_CHANGED(\n",
    "            input_size=(16, 96, 96, 4,4,12),\n",
    "            rank=embed_shape,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl=self.tcl_1,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout,\n",
    "                drop_path_rate=drop_path_rates[dpr_index + i]\n",
    "            )\n",
    "            for i in range(self.depths[0])\n",
    "        ])\n",
    "        dpr_index += self.depths[0]\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 96, 96, 4,4,12),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(4,4,24),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ------------------ Stage 2 Setup -------------------\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_2 = TCL_CHANGED(\n",
    "            input_size=(16, 48, 48, 4,4,24),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                tcl=self.tcl_2,\n",
    "                embed_shape=(4,4,24),\n",
    "                dropout=dropout,\n",
    "                drop_path_rate=drop_path_rates[dpr_index + i]\n",
    "            )\n",
    "            for i in range(self.depths[1])\n",
    "        ])\n",
    "        dpr_index += self.depths[1]\n",
    "\n",
    "        self.patch_merging_2 = TensorizedPatchMerging(\n",
    "            input_size=(16, 48, 48, 4,4,24),\n",
    "            in_embed_shape=(4,4,24),\n",
    "            out_embed_shape=(4,4,48),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ------------------ Stage 3 Setup -------------------\n",
    "        self.w_msa_3 = WindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.sw_msa_3 = ShiftedWindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_3 = TCL_CHANGED(\n",
    "            input_size=(16, 24, 24, 4,4,48),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block3_list = nn.ModuleList([\n",
    "            SwinBlock3(\n",
    "                w_msa=self.w_msa_3,\n",
    "                sw_msa=self.sw_msa_3,\n",
    "                tcl=self.tcl_3,\n",
    "                embed_shape=(4,4,48),\n",
    "                dropout=dropout,\n",
    "                drop_path_rate=drop_path_rates[dpr_index + i]\n",
    "            )\n",
    "            for i in range(self.depths[2])\n",
    "        ])\n",
    "        dpr_index += self.depths[2]\n",
    "\n",
    "        self.patch_merging_3 = TensorizedPatchMerging(\n",
    "            input_size=(16, 24, 24, 4,4,48),\n",
    "            in_embed_shape=(4,4,48),\n",
    "            out_embed_shape=(4,4,96),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ------------------ Stage 4 Setup -------------------\n",
    "        self.w_msa_4 = WindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.sw_msa_4 = ShiftedWindowMSA(\n",
    "            window_size=12,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_4 = TCL_CHANGED(\n",
    "            input_size=(16, 12, 7, 4,4,96),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block4_list = nn.ModuleList([\n",
    "            SwinBlock4(\n",
    "                w_msa=self.w_msa_4,\n",
    "                sw_msa=self.sw_msa_4,\n",
    "                tcl=self.tcl_4,\n",
    "                embed_shape=(4,4,96),\n",
    "                dropout=dropout,\n",
    "                drop_path_rate=drop_path_rates[dpr_index + i]\n",
    "            )\n",
    "            for i in range(self.depths[3])\n",
    "        ])\n",
    "        dpr_index += self.depths[3]\n",
    "\n",
    "        # ------------------ Classifier / Final Layer -------------------\n",
    "        self.classifier = TRL(\n",
    "            input_size=(16, 4, 4, 96),\n",
    "            output=(200,),\n",
    "            rank=(4,4,96,200),\n",
    "            ignore_modes=(0,),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ------------------ Position Embedding -------------------\n",
    "        # For example usage\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1, 96, 96, 4, 4, 12, device=self.device\n",
    "            ),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1) Patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # (2) Add position embedding\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # ----- Stage 1 -----\n",
    "        for blk in self.block1_list:\n",
    "            x = blk(x)\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "        # ----- Stage 2 -----\n",
    "        for blk in self.block2_list:\n",
    "            x = blk(x)\n",
    "        x = self.patch_merging_2(x)\n",
    "\n",
    "        # ----- Stage 3 -----\n",
    "        for blk in self.block3_list:\n",
    "            x = blk(x)\n",
    "        x = self.patch_merging_3(x)\n",
    "\n",
    "        # ----- Stage 4 -----\n",
    "        for blk in self.block4_list:\n",
    "            x = blk(x)\n",
    "\n",
    "        # Global average pooling (adapted for your tensor shape)\n",
    "        # In your snippet, you used x.mean(dim=(1, 2)), but your shape might be 6D.\n",
    "        # Adjust if necessary; for example, if your final shape is [B, H, W, ...], do:\n",
    "        x = x.mean(dim=(1, 2))  # Example: if shape is (B, 7, 7, D1, D2, D3)\n",
    "\n",
    "        # Final classifier\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID011'\n",
    "batch_size = 16\n",
    "n_epoch = 60\n",
    "image_size = 384\n",
    "\n",
    "model = SwinTransformer(\n",
    "        img_size=384,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_shape=(4,4,12),\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        max_drop_path=0.2,   # This is your maximum drop path probability\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "tiny_transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandAugment(),  # <- Added: Enables RandAugment \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "    transforms.RandomErasing(p=0.25)  # -> random_erasing with p=0.25\n",
    "])\n",
    "\n",
    "tiny_transform_val = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "tiny_transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "train_loader, val_loader , test_loader = get_tinyimagenet_dataloaders(\n",
    "                                                    data_dir = '../datasets',\n",
    "                                                    transform_train=tiny_transform_train,\n",
    "                                                    transform_val=tiny_transform_val,\n",
    "                                                    transform_test=tiny_transform_test,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.8):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda for mixup.\n",
    "    x: input images\n",
    "    y: labels\n",
    "    alpha: mixup alpha value, set to 0.8\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"Returns cutmixed inputs, pairs of targets, and lambda for cutmix.\n",
    "    x: input images\n",
    "    y: labels\n",
    "    alpha: cutmix alpha value, set to 1.0\n",
    "    \"\"\"\n",
    "    batch_size, _, H, W = x.size()\n",
    "    indices = torch.randperm(batch_size).to(x.device)\n",
    "    shuffled_x = x[indices]\n",
    "    shuffled_y = y[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    # Determine cut dimensions\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    # Randomly choose the center of the box\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    # Compute the bounding box coordinates and make sure they are within image bounds\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    # Replace region in original x with region from shuffled x\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = shuffled_x[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    return x, y, shuffled_y, lam_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 2424760 parameters\n",
      "Training for 60 epochs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 89\u001b[0m     report_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# report_test = test_epoch(test_loader, epoch)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     report \u001b[38;5;241m=\u001b[39m report_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#+ report_test + '\\n\\n'\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(loader, epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Use CutMix (cutmix_alpha = 1.0)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     inputs_aug, targets_a, targets_b, lam \u001b[38;5;241m=\u001b[39m cutmix_data(inputs, targets, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_aug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lam \u001b[38;5;241m*\u001b[39m criterion(outputs, targets_a) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lam) \u001b[38;5;241m*\u001b[39m criterion(outputs, targets_b)\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 234\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# ----- Stage 1 -----\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1_list:\n\u001b[1;32m--> 234\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_merging_1(x)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# ----- Stage 2 -----\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mSwinBlock1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m x_res \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x_res \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(x)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# (2) TCL + residual\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\Desktop\\tensorzied swin transformer main\\tensorized_swin_transformer\\TEST_EX\\..\\Tensorized_components\\w_msa_w_o_b_sign.py:209\u001b[0m, in \u001b[0;36mWindowMSA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    205\u001b[0m abs_attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(attn_flat)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# 8. Apply softmax over the key token dimension.\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m attn_softmax \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m attn_softmax \u001b[38;5;241m=\u001b[39m sign_attn \u001b[38;5;241m*\u001b[39m attn_softmax\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# print(\"attnetion softmax\")\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# print(attn_softmax.shape)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0}  # for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Randomly decide which augmentation to use on the mini-batch.\n",
    "        if np.random.rand() < 0.5:\n",
    "            # Use Mixup (mixup_alpha = 0.8)\n",
    "            inputs_aug, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=0.8)\n",
    "            outputs = model(inputs_aug)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        else:\n",
    "            # Use CutMix (cutmix_alpha = 1.0)\n",
    "            inputs_aug, targets_a, targets_b, lam = cutmix_data(inputs, targets, alpha=1.0)\n",
    "            outputs = model(inputs_aug)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # Optional: print progress for each batch\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k] / len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = (f'Train epoch {epoch}: top1={top1_acc*100:.2f}%, top2={top2_acc*100:.2f}%, '\n",
    "                    f'top3={top3_acc*100:.2f}%, top4={top4_acc*100:.2f}%, top5={top5_acc*100:.2f}%, '\n",
    "                    f'loss={avg_loss:.4f}, time={elapsed_time:.2f}s')\n",
    "    print(report_train)\n",
    "    return report_train\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
