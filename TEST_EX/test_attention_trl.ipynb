{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7c10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "# from Tensorized_Layers.TCL_CHANGED import TCL_CHANGED\n",
    "from Tensorized_Layers.TCL import TCL   as  TCL_CHANGED\n",
    "\n",
    "\n",
    "\n",
    "class WindowPartition(nn.Module):\n",
    "    \"\"\"\n",
    "    Utility module for partitioning and reversing windows in a patch grid.\n",
    "\n",
    "    Input shape: (B, H, W, *embed_dims)\n",
    "    After partitioning with a given window_size, the tensor is reshaped into:\n",
    "        (B, H//window_size, W//window_size, window_size, window_size, *embed_dims)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int):\n",
    "        super(WindowPartition, self).__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Partition the input tensor into non-overlapping windows.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, H, W, *embed_dims).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Partitioned tensor with shape \n",
    "                (B, H//window_size, W//window_size, window_size, window_size, *embed_dims).\n",
    "        \"\"\"\n",
    "        B, H, W, *embed_dims = x.shape\n",
    "        ws = self.window_size\n",
    "        if H % ws != 0 or W % ws != 0:\n",
    "            raise ValueError(\n",
    "                f\"H and W must be divisible by window_size {ws}. Got H={H}, W={W}.\")\n",
    "        # Reshape to split H and W into windows.\n",
    "        x = x.view(B, H // ws, ws, W // ws, ws, *embed_dims)\n",
    "        # Permute to group the window blocks together.\n",
    "        windows = x.permute(0, 1, 3, 2, 4, *range(5, x.dim()))\n",
    "        return windows\n",
    "\n",
    "    def reverse(self, windows: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse the window partition to reconstruct the original tensor.\n",
    "\n",
    "        Args:\n",
    "            windows (torch.Tensor): Partitioned windows with shape \n",
    "                (B, H//window_size, W//window_size, window_size, window_size, *embed_dims).\n",
    "            H (int): Original height.\n",
    "            W (int): Original width.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed tensor of shape (B, H, W, *embed_dims).\n",
    "        \"\"\"\n",
    "        ws = self.window_size\n",
    "        B, num_h, num_w, ws1, ws2, *embed_dims = windows.shape\n",
    "        # Permute back to interleave the window dimensions.\n",
    "        x = windows.permute(\n",
    "            0, 1, 3, 2, 4, *range(5, windows.dim())).contiguous()\n",
    "        # Reshape to reconstruct the original feature map.\n",
    "        x = x.view(B, num_h * ws1, num_w * ws2, *embed_dims)\n",
    "        return x\n",
    "\n",
    "class WindowMSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multi-Head Self-Attention (W-MSA) module.\n",
    "\n",
    "    This module partitions the input tensor into windows, computes tensorized Q, K, V\n",
    "    using TCL layers (which operate on each window), applies relative positional bias,\n",
    "    computes self-attention scores, and reconstructs the full feature map.\n",
    "\n",
    "    Args:\n",
    "        window_size (int): Spatial size of the window (e.g., 7).\n",
    "        embed_dims (tuple): Embedding dimensions for each patch (e.g., (4, 4, 3)).\n",
    "        rank_window (tuple): Output dimensions from TCL layers for each window \n",
    "                             (e.g., (4, 4, 3)). These should be divisible by the\n",
    "                             corresponding head factors.\n",
    "        head_factors (tuple): Factors to split the TCL output channels into heads.\n",
    "                              For example, (2, 2, 1) will yield 2*2*1 = 4 heads.\n",
    "        device (str): Device identifier (default 'cpu').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int, embed_dims: tuple, rank_window: tuple, head_factors: tuple, device='cpu'):\n",
    "        super(WindowMSA, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embed_dims = embed_dims      # e.g., (4, 4, 3)\n",
    "        self.rank_window = rank_window    # e.g., (4, 4, 3)\n",
    "        self.head_factors = head_factors  # e.g., (2, 2, 1)\n",
    "\n",
    "        self.device = device\n",
    "        # Number of heads is the product of the head factors.\n",
    "\n",
    "        self.scale = ((self.embed_dims[0] // self.head_factors[0]) *\n",
    "                      (self.embed_dims[1] // self.head_factors[1]) *\n",
    "                      (self.embed_dims[2] // self.head_factors[2])) ** (-0.5)\n",
    "\n",
    "        self.num_heads = 1\n",
    "        for h in head_factors:\n",
    "            self.num_heads *= h\n",
    "\n",
    "        # Input size for TCL layers for each window: (window_size, window_size, *embed_dims)\n",
    "        self.input_size_window = (window_size, window_size) + embed_dims\n",
    "\n",
    "        # Instantiate TCL layers for Q, K, and V.\n",
    "\n",
    "        self.tcl_q = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1,2), bias=True, device=self.device)\n",
    "        self.tcl_k = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1,2), bias=True, device=self.device)\n",
    "        self.tcl_v = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1,2), bias=True, device=self.device)\n",
    "\n",
    "\n",
    "        self.window_partition = WindowPartition(window_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, *embed_shape = x.shape\n",
    "        ws = self.window_size\n",
    "\n",
    "        x_windows = self.window_partition(x)\n",
    "        B, nH, nW, ws1, ws2, *_ = x_windows.shape  \n",
    "        num_windows = nH * nW\n",
    "\n",
    "        x_windows = x_windows.reshape(B * num_windows, ws1, ws2, *embed_shape)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"shape of x windows is:\" , x_windows.shape)\n",
    "\n",
    "\n",
    "        Q_windows = self.tcl_q(x_windows)\n",
    "        K_windows = self.tcl_k(x_windows)\n",
    "        V_windows = self.tcl_v(x_windows)\n",
    "\n",
    "\n",
    "        print(\"shape of  Q windows is:\" , Q_windows.shape)\n",
    "\n",
    "        Q_windows = Q_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "        K_windows = K_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "        V_windows = V_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "\n",
    "        h1, h2, h3 = self.head_factors  # e.g., (2, 2, 1)\n",
    "        r1, r2, r3 = self.rank_window     # e.g., (4, 4, 3)\n",
    "        q = rearrange(Q_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "        k = rearrange(K_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "        v = rearrange(V_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "\n",
    "\n",
    "        attn = torch.einsum(\n",
    "            \"b m n i j a d e x y z, b m n k l a d e x y z -> b m n a d e i j k l\",\n",
    "            q, k\n",
    "        ) * self.scale\n",
    "\n",
    "        num_tokens = ws1 * ws2\n",
    "\n",
    "\n",
    "        attn_flat = attn.view(B, nH, nW, h1, h2, h3, num_tokens, num_tokens)\n",
    "        \n",
    "        attn_flat = attn_flat \n",
    "\n",
    "        attn_softmax = torch.softmax(attn_flat, dim=-1)\n",
    "\n",
    "        attn = attn_softmax.view(B, nH, nW, h1, h2, h3, ws1, ws2, ws1, ws2)\n",
    "\n",
    "        final_output = torch.einsum(\n",
    "            \"b m n a d e i j k l, b m n i j a d e x y z -> b m n a d e k l x y z\",\n",
    "            attn, v\n",
    "        )\n",
    "\n",
    "\n",
    "        final_output_reshaped = rearrange(\n",
    "            final_output, \"b m n a d e k l x y z -> b m n k l (a x) (d y) (e z)\"\n",
    "        )\n",
    "\n",
    "\n",
    "        out = self.window_partition.reverse(final_output_reshaped, H, W)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec41fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m rank_window \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      7\u001b[0m head_factors \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m w_msa \u001b[38;5;241m=\u001b[39m \u001b[43mWindowMSA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrank_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_factors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m w_msa(x)\n",
      "Cell \u001b[1;32mIn[1], line 111\u001b[0m, in \u001b[0;36mWindowMSA.__init__\u001b[1;34m(self, window_size, embed_dims, rank_window, head_factors, device)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_window \u001b[38;5;241m=\u001b[39m (window_size, window_size) \u001b[38;5;241m+\u001b[39m embed_dims\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Instantiate TCL layers for Q, K, and V.\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcl_q \u001b[38;5;241m=\u001b[39m \u001b[43mTCL_CHANGED\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_modes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcl_k \u001b[38;5;241m=\u001b[39m TCL_CHANGED(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_window,\n\u001b[0;32m    114\u001b[0m                          rank\u001b[38;5;241m=\u001b[39mrank_window, ignore_modes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcl_v \u001b[38;5;241m=\u001b[39m TCL_CHANGED(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_window,\n\u001b[0;32m    116\u001b[0m                          rank\u001b[38;5;241m=\u001b[39mrank_window, ignore_modes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\Desktop\\university\\swin github\\001\\tensorized_swin_transformer\\TEST_EX\\..\\Tensorized_Layers\\TCL.py:51\u001b[0m, in \u001b[0;36mTCL.__init__\u001b[1;34m(self, input_size, rank, ignore_modes, bias, device)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Tucker Decomposition method for TCL\u001b[39;00m\n\u001b[0;32m     48\u001b[0m                            \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# List of all factors\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty((r, \u001b[43mnew_size\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m), device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Generate formula for output :\u001b[39;00m\n\u001b[0;32m     54\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "x = torch.randn(1, 56, 56, 4, 4, 3, device=\"cpu\")\n",
    "\n",
    "window_size = 7\n",
    "\n",
    "rank_window = (4, 4, 3)\n",
    "head_factors = (2, 2, 1)\n",
    "\n",
    "w_msa = WindowMSA(window_size=window_size, embed_dims=(4, 4, 3),\n",
    "                    rank_window=rank_window, head_factors=head_factors, device=\"cpu\").to(\"cpu\")\n",
    "\n",
    "x = w_msa(x)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
