{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "# convolution patch embedding\n",
    "# from Tensorized_components.patch_embedding  import Patch_Embedding        \n",
    "from Tensorized_components.tcl_patch_embedding  import  PatchEmbedding  as  Patch_Embedding      \n",
    "from Tensorized_components.w_msa_w_o_b  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL  import TCL  as TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.cifar100_loaders import get_cifar100_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing 'Block 1' in your Swin Transformer.\n",
    "    This captures the sequence of:\n",
    "        (1) Window MSA + residual\n",
    "        (2) TCL + residual\n",
    "        (3) Shifted Window MSA + residual\n",
    "        (4) TCL + residual\n",
    "    but only for the first blockâ€™s hyperparameters and submodules.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,tcl2,tcl3,tcl4, embed_shape=(3,4,4), dropout=0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        # Typically each sub-layer has its own LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We pass in pre-built modules (WindowMSA, ShiftedWindowMSA, TCL)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "    def forward(self, x):\n",
    "        # ----- First Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- Shifted Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4,  embed_shape=(3,4,8), dropout=0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "        # LN layers\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # Shifted Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(3,4,4),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "# tcl patch embedding \n",
    "\n",
    "\n",
    "# TODO : change batch size and device to cuda\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            input_size=(16,3,32,32),\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_shape,\n",
    "            bias=bias,\n",
    "            device=\"cpu\",\n",
    "            ignore_modes = (0,1,2)\n",
    "        )\n",
    "# convolution \n",
    "        # self.patch_embedding = Patch_Embedding(\n",
    "        #     img_size=img_size,\n",
    "        #     patch_size=patch_size,\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_shape=embed_shape,\n",
    "        #     bias=bias\n",
    "        # )\n",
    "\n",
    "        # -------------------------------- block 1 --------------------------\n",
    "\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=4,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=4,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1 = TCL_CHANGED(\n",
    "            input_size=(16, 8, 8, 3,4,4),\n",
    "            rank=(3,4,16),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_2 = TCL_CHANGED(\n",
    "            input_size=(16, 8, 8, 3,4,16),\n",
    "            rank=(3,4,4),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_3 =  TCL_CHANGED(\n",
    "            input_size=(16, 8, 8, 3,4,4),\n",
    "            rank=(3,4,16),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_1_4 = TCL_CHANGED(\n",
    "            input_size=(16, 8, 8, 3,4,16),\n",
    "            rank=(3,4,4),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl1=self.tcl_1,\n",
    "                tcl2 = self.tcl_1_2,\n",
    "                tcl3 = self.tcl_1_3,\n",
    "                tcl4 = self.tcl_1_4,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- block 2 --------------------------\n",
    "\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 8, 8, 3,4,4),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(3,4,8),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=4,\n",
    "            embed_dims=(3,4,8),\n",
    "            rank_window=(3,4,8),\n",
    "            head_factors=(1,2,4),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=4,\n",
    "            embed_dims=(3,4,8),\n",
    "            rank_window=(3,4,8),\n",
    "            head_factors=(1,2,4),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2 = TCL_CHANGED(\n",
    "            input_size=(16, 4, 4, 3,4,8),\n",
    "            rank=(3,4,32),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.tcl_2_2 = TCL_CHANGED(\n",
    "            input_size=(16, 4, 4, 3,4,32),\n",
    "            rank=(3,4,8),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2_3 = TCL_CHANGED(\n",
    "            input_size=(16, 4, 4, 3,4,8),\n",
    "            rank=(3,4,32),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.tcl_2_4 = TCL_CHANGED(\n",
    "            input_size=(16, 4, 4, 3,4,32),\n",
    "            rank=(3,4,8),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # We repeat Block2 two times\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                tcl1=self.tcl_2,\n",
    "                tcl2 = self.tcl_2_2,\n",
    "                tcl3 = self.tcl_2_3,\n",
    "                tcl4 = self.tcl_2_4,\n",
    "                embed_shape=(3,4,8),  \n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # -------------------------------- classifier --------------------------\n",
    "\n",
    "    \n",
    "\n",
    "        self.classifier = TRL(input_size=(16,3,4,8),\n",
    "                            output=(200,),\n",
    "                            rank=(3,4,8,200),\n",
    "                            ignore_modes=(0,),\n",
    "                            bias=bias,\n",
    "                            device=self.device) \n",
    "        \n",
    "\n",
    "        # positoin embedding\n",
    "\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1,\n",
    "                        8,\n",
    "                        8,\n",
    "                        3,\n",
    "                        4,\n",
    "                        4,\n",
    "                        device = self.device\n",
    "                        ), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for i, blk in enumerate(self.block1_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "        for i, blk in enumerate(self.block2_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed) \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=0.8):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_epochs, num_training_epochs):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < num_warmup_epochs:\n",
    "            return float(epoch) / float(max(1, num_warmup_epochs))\n",
    "        return 0.5 * (1. + np.cos(np.pi * (epoch - num_warmup_epochs) / (num_training_epochs - num_warmup_epochs)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cpu\n",
      "No seed is set!\n"
     ]
    }
   ],
   "source": [
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "\n",
    "SEED = None \n",
    "\n",
    "if SEED is None:\n",
    "    print(f'No seed is set!')\n",
    "else:\n",
    "    set_seed(seed=SEED)\n",
    "\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID00091'\n",
    "batch_size = 256\n",
    "n_epoch = 400\n",
    "image_size = 32\n",
    "\n",
    "model = SwinTransformer(img_size=32,patch_size=4,in_chans=3,embed_shape=(3,4,4),bias=True,device=device).to(device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "            RandAugment(), \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            RandomErasing(p=0.25)\n",
    "        ])\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "\n",
    "# train_size = 4\n",
    "train_loader, _ = get_cifar100_dataloaders('../datasets', transform_train, transform_test, batch_size, image_size, train_size, repeat_count=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 72391 parameters\n",
      "Training for 400 epochs\n",
      "\n",
      "Train epoch 1: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.273856520652771, time=0.2713048458099365s\n",
      "Train epoch 2: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2737565517425537, time=0.32888174057006836s\n",
      "Train epoch 3: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27365598678588865, time=0.2349529266357422s\n",
      "Train epoch 4: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2735543727874756, time=0.2446751594543457s\n",
      "Train epoch 5: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27345116138458253, time=0.21798014640808105s\n",
      "Train epoch 6: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2733457088470459, time=0.24177885055541992s\n",
      "Train epoch 7: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2732372283935547, time=0.21313762664794922s\n",
      "Train epoch 8: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27312483787536623, time=0.20009279251098633s\n",
      "Train epoch 9: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2730071783065796, time=0.20839548110961914s\n",
      "Train epoch 10: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2728828191757202, time=0.20953726768493652s\n",
      "Train epoch 11: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27274973392486573, time=0.2410259246826172s\n",
      "Train epoch 12: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2726057291030884, time=0.24855446815490723s\n",
      "Train epoch 13: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2724473476409912, time=0.25096988677978516s\n",
      "Train epoch 14: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2722717523574829, time=0.2329697608947754s\n",
      "Train epoch 15: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2720733642578125, time=0.26706409454345703s\n",
      "Train epoch 16: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2718472957611084, time=0.23413753509521484s\n",
      "Train epoch 17: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27158656120300295, time=0.21743440628051758s\n",
      "Train epoch 18: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27128100395202637, time=0.24090313911437988s\n",
      "Train epoch 19: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.27092416286468507, time=0.2674224376678467s\n",
      "Train epoch 20: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2705008745193481, time=0.24860024452209473s\n",
      "Train epoch 21: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.26999735832214355, time=0.1975080966949463s\n",
      "Train epoch 22: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2693928241729736, time=0.20615458488464355s\n",
      "Train epoch 23: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.26867103576660156, time=0.24968385696411133s\n",
      "Train epoch 24: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2678055763244629, time=0.24982714653015137s\n",
      "Train epoch 25: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2667612314224243, time=0.23126459121704102s\n",
      "Train epoch 26: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2655038356781006, time=0.2574903964996338s\n",
      "Train epoch 27: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2639936923980713, time=0.26729655265808105s\n",
      "Train epoch 28: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.0%, top5=0.0%, loss=0.2621780872344971, time=0.2664163112640381s\n",
      "Train epoch 29: top1=0.0%, top2=0.0%, top3=0.0%, top4=0.25%, top5=0.25%, loss=0.2599958419799805, time=0.29110002517700195s\n",
      "Train epoch 30: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.2573706865310669, time=0.28936290740966797s\n",
      "Train epoch 31: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.25422675609588624, time=0.25283241271972656s\n",
      "Train epoch 32: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.25047199726104735, time=0.2409358024597168s\n",
      "Train epoch 33: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.24599070549011232, time=0.22755074501037598s\n",
      "Train epoch 34: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.24064602851867675, time=0.21562671661376953s\n",
      "Train epoch 35: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.23431541919708251, time=0.22464680671691895s\n",
      "Train epoch 36: top1=0.25%, top2=0.25%, top3=0.25%, top4=0.25%, top5=0.25%, loss=0.22679696083068848, time=0.24207520484924316s\n",
      "Train epoch 37: top1=0.25%, top2=0.5%, top3=0.75%, top4=0.75%, top5=0.75%, loss=0.21795878410339356, time=0.23485684394836426s\n",
      "Train epoch 38: top1=0.25%, top2=0.5%, top3=0.75%, top4=0.75%, top5=0.75%, loss=0.2075958490371704, time=0.22469449043273926s\n",
      "Train epoch 39: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.19549576044082642, time=0.25012969970703125s\n",
      "Train epoch 40: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.18158206939697266, time=0.268862247467041s\n",
      "Train epoch 41: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.1657738447189331, time=0.27269721031188965s\n",
      "Train epoch 42: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.14831254482269288, time=0.2248990535736084s\n",
      "Train epoch 43: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.12988306283950807, time=0.23906707763671875s\n",
      "Train epoch 44: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.11185586452484131, time=0.22194838523864746s\n",
      "Train epoch 45: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.09635804891586304, time=0.24428081512451172s\n",
      "Train epoch 46: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.08517222404479981, time=0.24312853813171387s\n",
      "Train epoch 47: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07842944860458374, time=0.2368617057800293s\n",
      "Train epoch 48: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07463001012802124, time=0.25356030464172363s\n",
      "Train epoch 49: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0723267912864685, time=0.2637472152709961s\n",
      "Train epoch 50: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07078853845596314, time=0.22431588172912598s\n",
      "Train epoch 51: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06981854438781739, time=0.2529482841491699s\n",
      "Train epoch 52: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06945894956588745, time=0.22824788093566895s\n",
      "Train epoch 53: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06971038579940796, time=0.24025368690490723s\n",
      "Train epoch 54: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07042748928070068, time=0.23651552200317383s\n",
      "Train epoch 55: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07130104303359985, time=0.22245049476623535s\n",
      "Train epoch 56: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07201386094093323, time=0.19077110290527344s\n",
      "Train epoch 57: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07231715321540833, time=0.19405007362365723s\n",
      "Train epoch 58: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0721555233001709, time=0.20350360870361328s\n",
      "Train epoch 59: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07157254219055176, time=0.23090100288391113s\n",
      "Train epoch 60: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07076331377029418, time=0.23827719688415527s\n",
      "Train epoch 61: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06997466087341309, time=0.26409435272216797s\n",
      "Train epoch 62: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06944183111190796, time=0.25667858123779297s\n",
      "Train epoch 63: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06933308839797973, time=0.25481319427490234s\n",
      "Train epoch 64: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06965197324752807, time=0.30933213233947754s\n",
      "Train epoch 65: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07017465233802796, time=0.26955556869506836s\n",
      "Train epoch 66: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07056153416633607, time=0.3230414390563965s\n",
      "Train epoch 67: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07060075998306274, time=0.3236734867095947s\n",
      "Train epoch 68: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.07031574845314026, time=0.3540384769439697s\n",
      "Train epoch 69: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.069892418384552, time=0.3413724899291992s\n",
      "Train epoch 70: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06953243613243103, time=0.31719136238098145s\n",
      "Train epoch 71: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06936565637588502, time=0.2665369510650635s\n",
      "Train epoch 72: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06940025091171265, time=0.25031423568725586s\n",
      "Train epoch 73: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06956211924552917, time=0.27487993240356445s\n",
      "Train epoch 74: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06975033283233642, time=0.3396458625793457s\n",
      "Train epoch 75: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06985881328582763, time=0.3273744583129883s\n",
      "Train epoch 76: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06983386874198913, time=0.3177947998046875s\n",
      "Train epoch 77: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06969165802001953, time=0.3518509864807129s\n",
      "Train epoch 78: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06950268745422364, time=0.30274081230163574s\n",
      "Train epoch 79: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06935971975326538, time=0.30586981773376465s\n",
      "Train epoch 80: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06931211352348328, time=0.3255150318145752s\n",
      "Train epoch 81: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06936625838279724, time=0.31099486351013184s\n",
      "Train epoch 82: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06946341991424561, time=0.3453812599182129s\n",
      "Train epoch 83: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0695402204990387, time=0.32898902893066406s\n",
      "Train epoch 84: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06955205202102661, time=0.3116445541381836s\n",
      "Train epoch 85: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06949651837348939, time=0.38538694381713867s\n",
      "Train epoch 86: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06941388845443726, time=0.5284109115600586s\n",
      "Train epoch 87: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06934015154838562, time=0.3233072757720947s\n",
      "Train epoch 88: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06930850744247437, time=0.29402780532836914s\n",
      "Train epoch 89: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06932780146598816, time=0.27355384826660156s\n",
      "Train epoch 90: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06937109231948853, time=0.2627089023590088s\n",
      "Train epoch 91: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06940793991088867, time=0.3161625862121582s\n",
      "Train epoch 92: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06941605210304261, time=0.31566929817199707s\n",
      "Train epoch 93: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0693928837776184, time=0.3530571460723877s\n",
      "Train epoch 94: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06935445070266724, time=0.3181619644165039s\n",
      "Train epoch 95: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06932201385498046, time=0.273207426071167s\n",
      "Train epoch 96: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06931034922599792, time=0.260145902633667s\n",
      "Train epoch 97: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06931937336921692, time=0.24184346199035645s\n",
      "Train epoch 98: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06933788061141968, time=0.25286436080932617s\n",
      "Train epoch 99: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06935240030288696, time=0.25519561767578125s\n",
      "Train epoch 100: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06935247778892517, time=0.29915952682495117s\n",
      "Train epoch 101: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06934401392936707, time=0.29560112953186035s\n",
      "Train epoch 102: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06932703852653503, time=0.3376774787902832s\n",
      "Train epoch 103: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06931197047233581, time=0.3303203582763672s\n",
      "Train epoch 104: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0693092703819275, time=0.3247396945953369s\n",
      "Train epoch 105: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06931355595588684, time=0.3788876533508301s\n",
      "Train epoch 106: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06932138204574585, time=0.3237490653991699s\n",
      "Train epoch 107: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0693287968635559, time=0.3416886329650879s\n",
      "Train epoch 108: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0693306565284729, time=0.3145284652709961s\n",
      "Train epoch 109: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06932309865951539, time=0.33381009101867676s\n",
      "Train epoch 110: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.0693170189857483, time=0.3772916793823242s\n",
      "Train epoch 111: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06930976510047912, time=0.3185701370239258s\n",
      "Train epoch 112: top1=0.25%, top2=0.5%, top3=0.75%, top4=1.0%, top5=1.0%, loss=0.06930606961250305, time=0.29566240310668945s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 82\u001b[0m     report_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# report_test = test_epoch(test_loader, epoch)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     report \u001b[38;5;241m=\u001b[39m report_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#+ report_test + '\\n\\n'\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(loader, epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 214\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    211\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1_list, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_merging_1(x)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2_list, \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m, in \u001b[0;36mSwinBlock1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# ----- Shifted Window MSA + Residual -----\u001b[39;00m\n\u001b[0;32m     46\u001b[0m x_res \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm3\u001b[49m(x)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msw_msa(x))\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_res\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[1;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Define train and test functions (use examples)\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # print(f'batch{i} done!')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = f'Train epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_train)\n",
    "\n",
    "    return report_train\n",
    "\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
