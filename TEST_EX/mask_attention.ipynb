{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final attention mask shape: torch.Size([1, 2, 2, 1, 1, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_2d_attention_mask(H=8, W=8, window_size=4, shift_size=2):\n",
    "    \"\"\"\n",
    "    Generates a 2D attention mask, ending with shape:\n",
    "      [1, num_win_h, num_win_w, 1, 1, 1, window_size*window_size, window_size*window_size]\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 1) Create a label mask [1, H, W, 1] and fill it using \"shifted\" slices\n",
    "    # --------------------------------------------------------------------------\n",
    "    img_mask = torch.zeros((1, H, W, 1))\n",
    "\n",
    "    h_slices = (\n",
    "        slice(0, -window_size),\n",
    "        slice(-window_size, -shift_size),\n",
    "        slice(-shift_size,  None)\n",
    "    )\n",
    "    w_slices = (\n",
    "        slice(0, -window_size),\n",
    "        slice(-window_size, -shift_size),\n",
    "        slice(-shift_size,  None)\n",
    "    )\n",
    "\n",
    "    cnt = 0\n",
    "    for h in h_slices:\n",
    "        for w in w_slices:\n",
    "            img_mask[:, h, w, :] = cnt\n",
    "            cnt += 1\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 2) Window partition\n",
    "    # --------------------------------------------------------------------------\n",
    "    def window_partition_mask(x, window_size):\n",
    "        \"\"\"\n",
    "        Partitions x into non-overlapping windows of size (window_size x window_size).\n",
    "        Returns:\n",
    "            windows: shape [num_windows * B, window_size, window_size, C]\n",
    "        \"\"\"\n",
    "        B, H_, W_, C = x.shape\n",
    "        # Reshape into windows\n",
    "        x = x.view(\n",
    "            B,\n",
    "            H_ // window_size, window_size,\n",
    "            W_ // window_size, window_size,\n",
    "            C\n",
    "        )\n",
    "        # Permute to group each window in its own batch dimension\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        windows = x.view(-1, window_size, window_size, C)\n",
    "        return windows\n",
    "\n",
    "    # Partition into windows\n",
    "    mask_windows = window_partition_mask(img_mask, window_size)\n",
    "    # mask_windows => [num_windows, window_size, window_size, 1]\n",
    "\n",
    "    # Flatten each window\n",
    "    mask_windows = mask_windows.view(-1, window_size * window_size)\n",
    "    # => [num_windows, window_size*window_size]\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 3) Build the attention mask by comparing window-patch labels\n",
    "    # --------------------------------------------------------------------------\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0))\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 4) Reshape to the final shape [1, num_win_h, num_win_w, 1, 1, 1, 49, 49]\n",
    "    #    for the 56x56, window_size=7 example.\n",
    "    # --------------------------------------------------------------------------\n",
    "    num_win_h = H // window_size\n",
    "    num_win_w = W // window_size\n",
    "\n",
    "    # 4a) First add a batch dimension\n",
    "    attn_mask = attn_mask.unsqueeze(0)  # => [1, num_windows, window_size*window_size, window_size*window_size]\n",
    "\n",
    "    # 4b) Reshape num_windows into (num_win_h, num_win_w)\n",
    "    attn_mask = attn_mask.reshape(\n",
    "        1,\n",
    "        num_win_h,\n",
    "        num_win_w,\n",
    "        window_size * window_size,\n",
    "        window_size * window_size\n",
    "    )\n",
    "    # => [1, num_win_h, num_win_w, 49, 49] for window_size=7\n",
    "\n",
    "    # 4c) Finally, insert three singleton dimensions in the middle\n",
    "    #     => [1, num_win_h, num_win_w, 1, 1, 1, 49, 49]\n",
    "    attn_mask = attn_mask.unsqueeze(3).unsqueeze(4).unsqueeze(5)\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    attn_mask_function = generate_2d_attention_mask(\n",
    "        H=8, W=8, window_size=4, shift_size=2\n",
    "    )\n",
    "    print(\"Final attention mask shape:\", attn_mask_function.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
