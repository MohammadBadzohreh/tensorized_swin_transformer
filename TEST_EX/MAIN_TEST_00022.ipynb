{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "# convolution patch embedding\n",
    "# from Tensorized_components.patch_embedding  import Patch_Embedding        \n",
    "from Tensorized_components.tcl_patch_embedding  import  PatchEmbedding  as  Patch_Embedding      \n",
    "from Tensorized_components.w_msa_w_o_b_sign  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b_sign import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL  import TCL  as TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "from Utils.Num_parameter import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing 'Block 1' in your Swin Transformer.\n",
    "    This captures the sequence of:\n",
    "        (1) Window MSA + residual\n",
    "        (2) TCL + residual\n",
    "        (3) Shifted Window MSA + residual\n",
    "        (4) TCL + residual\n",
    "    but only for the first blockâ€™s hyperparameters and submodules.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,tcl2,tcl3,tcl4, embed_shape, dropout=0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        # Typically each sub-layer has its own LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We pass in pre-built modules (WindowMSA, ShiftedWindowMSA, TCL)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "    def forward(self, x):\n",
    "        # ----- First Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- Shifted Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4,  embed_shape=(4,4,6), dropout=0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "        # LN layers\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # Shifted Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock3(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,  tcl2 , tcl3 , tcl4,   embed_shape=(4,4,12), dropout=0):\n",
    "        super(SwinBlock3, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock4(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4 ,  embed_shape=(4,4,24), dropout=0):\n",
    "        super(SwinBlock4, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "# tcl patch embedding \n",
    "\n",
    "\n",
    "# TODO : change batch size and device to cuda\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            input_size=(16,3,224,224),\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_shape,\n",
    "            bias=bias,\n",
    "            device=\"cpu\",\n",
    "            ignore_modes = (0,1,2)\n",
    "        )\n",
    "# convolution \n",
    "        # self.patch_embedding = Patch_Embedding(\n",
    "        #     img_size=img_size,\n",
    "        #     patch_size=patch_size,\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_shape=embed_shape,\n",
    "        #     bias=bias\n",
    "        # )\n",
    "\n",
    "        # -------------------------------- block 1 --------------------------\n",
    "\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_2 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_3 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_4 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl1=self.tcl_1,\n",
    "                tcl2 = self.tcl_1_2,\n",
    "                tcl3 = self.tcl_1_3,\n",
    "                tcl4 = self.tcl_1_4,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- block 2 --------------------------\n",
    "\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(4,4,24),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.tcl_2_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2_3 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.tcl_2_4 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # We repeat Block2 two times\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                tcl1=self.tcl_2,\n",
    "                tcl2 = self.tcl_2_2,\n",
    "                tcl3 = self.tcl_2_3,\n",
    "                tcl4 = self.tcl_2_4,\n",
    "                embed_shape=(4,4,24),  \n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # # -------------------------------- block 3 --------------------------\n",
    "\n",
    "        self.patch_merging_2 = TensorizedPatchMerging(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            in_embed_shape=(4,4,24),\n",
    "            out_embed_shape=(4,4,48),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.w_msa_3 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_3 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_3_2 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_4 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Repeat Block3 6 times\n",
    "        self.block3_list = nn.ModuleList([\n",
    "            SwinBlock3(\n",
    "                w_msa=self.w_msa_3,\n",
    "                sw_msa=self.sw_msa_3,\n",
    "                tcl1=self.tcl_3,\n",
    "                tcl2=self.tcl_3_2,\n",
    "                tcl3=self.tcl_3_3,\n",
    "                tcl4=self.tcl_3_4,\n",
    "                embed_shape=(4,4,48),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(18)\n",
    "        ])\n",
    "\n",
    "        # # # -------------------------------- block 4 --------------------------\n",
    "\n",
    "        self.patch_merging_3 = TensorizedPatchMerging(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            in_embed_shape=(4,4,48),\n",
    "            out_embed_shape=(4,4,96),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_4 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_4 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4_2 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_3 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block4_list = nn.ModuleList([\n",
    "            SwinBlock4(\n",
    "                w_msa=self.w_msa_4,\n",
    "                sw_msa=self.sw_msa_4,\n",
    "                tcl1=self.tcl_4,\n",
    "                tcl2=self.tcl_4_2,\n",
    "                tcl3=self.tcl_4_3,\n",
    "                tcl4=self.tcl_4_4,\n",
    "                embed_shape=(4,4,96),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- classifier --------------------------\n",
    "\n",
    "    \n",
    "\n",
    "        self.classifier = TRL(input_size=(16,4,4,96),\n",
    "                            output=(200,),\n",
    "                            rank=(4,4,96,200),\n",
    "                            ignore_modes=(0,),\n",
    "                            bias=bias,\n",
    "                            device=self.device) \n",
    "        \n",
    "\n",
    "        # positoin embedding\n",
    "\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1,\n",
    "                        56,\n",
    "                        56,\n",
    "                        4,\n",
    "                        4,\n",
    "                        12,\n",
    "                        device = self.device\n",
    "                        ), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for i, blk in enumerate(self.block1_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block2_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_2(x)\n",
    "\n",
    "        for i, blk in enumerate(self.block3_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_3(x)\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block4_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID00022'\n",
    "batch_size = 16\n",
    "n_epoch = 400\n",
    "image_size = 224\n",
    "\n",
    "model = SwinTransformer(img_size=224,patch_size=4,in_chans=3,embed_shape=(4,4,12),bias=True,device=device).to(device)\n",
    "\n",
    "\n",
    "# Set up the transforms and train/test loaders\n",
    "\n",
    "tiny_transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.RandomCrop(image_size, padding=5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_val = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "tiny_transform_test = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "\n",
    "train_loader, val_loader , test_loader = get_tinyimagenet_dataloaders(\n",
    "                                                    data_dir = '../datasets',\n",
    "                                                    transform_train=tiny_transform_train,\n",
    "                                                    transform_val=tiny_transform_val,\n",
    "                                                    transform_test=tiny_transform_test,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 1700556 parameters\n",
      "Training for 400 epochs\n",
      "\n",
      "Train epoch 1: top1=0.0127399992197752%, top2=0.024960000067949295%, top3=0.03677999973297119%, top4=0.04797999933362007%, top5=0.0587799996137619%, loss=0.3222696713829041, time=2102.369265794754s\n",
      "Train epoch 2: top1=0.05308999866247177%, top2=0.09088999778032303%, top3=0.12282999604940414%, top4=0.15090999007225037%, top5=0.17662999033927917%, loss=0.2928925315475464, time=2102.8262679576874s\n",
      "Train epoch 3: top1=0.093469999730587%, top2=0.14778999984264374%, top3=0.18978999555110931%, top4=0.22562000155448914%, top5=0.256630003452301%, loss=0.27386309999227526, time=2101.132833480835s\n",
      "Train epoch 4: top1=0.12457999587059021%, top2=0.19070999324321747%, top3=0.23952999711036682%, top4=0.27903997898101807%, top5=0.31296998262405396%, loss=0.25956331155061724, time=2101.830500602722s\n",
      "Train epoch 5: top1=0.15226998925209045%, top2=0.2275400012731552%, top3=0.2811799943447113%, top4=0.32516998052597046%, top5=0.36087000370025635%, loss=0.24774873340129852, time=2100.1406710147858s\n",
      "Train epoch 6: top1=0.17260999977588654%, top2=0.2555299997329712%, top3=0.3127399981021881%, top4=0.35753998160362244%, top5=0.3942199945449829%, loss=0.2390806670475006, time=2103.1813747882843s\n",
      "Train epoch 7: top1=0.19130998849868774%, top2=0.2790699899196625%, top3=0.33820000290870667%, top4=0.3845999836921692%, top5=0.42152997851371765%, loss=0.23158400648355484, time=2101.6280007362366s\n",
      "Train epoch 8: top1=0.20778998732566833%, top2=0.29892000555992126%, top3=0.36090999841690063%, top4=0.4082399904727936%, top5=0.44620999693870544%, loss=0.22492073977947236, time=2105.025366783142s\n",
      "Train epoch 9: top1=0.22530999779701233%, top2=0.3184399902820587%, top3=0.38078999519348145%, top4=0.4282200038433075%, top5=0.46719998121261597%, loss=0.21860042164564134, time=2102.7427937984467s\n",
      "Train epoch 10: top1=0.24086999893188477%, top2=0.3382200002670288%, top3=0.4011099934577942%, top4=0.4493899941444397%, top5=0.48812997341156006%, loss=0.21295285771369935, time=2092.0874218940735s\n",
      "Train epoch 11: top1=0.2534500062465668%, top2=0.35450997948646545%, top3=0.4200199842453003%, top4=0.46852999925613403%, top5=0.5069400072097778%, loss=0.20767927012085916, time=2085.5636491775513s\n",
      "Train epoch 12: top1=0.26912999153137207%, top2=0.37217000126838684%, top3=0.4373299777507782%, top4=0.48596999049186707%, top5=0.5264700055122375%, loss=0.20223203724861144, time=2084.7086160182953s\n",
      "Train epoch 13: top1=0.28237998485565186%, top2=0.387939989566803%, top3=0.45419999957084656%, top4=0.5038099884986877%, top5=0.5422300100326538%, loss=0.19725820720553397, time=2085.1597261428833s\n",
      "Train epoch 14: top1=0.2937999963760376%, top2=0.4030799865722656%, top3=0.47151997685432434%, top4=0.5202299952507019%, top5=0.5584799647331238%, loss=0.192702581564188, time=2084.497426509857s\n",
      "Train epoch 15: top1=0.3073999881744385%, top2=0.41721999645233154%, top3=0.48489999771118164%, top4=0.5342000126838684%, top5=0.5729699730873108%, loss=0.18813758437395095, time=2088.352968931198s\n",
      "Train epoch 16: top1=0.3205699920654297%, top2=0.4321399927139282%, top3=0.4998199939727783%, top4=0.5485699772834778%, top5=0.5870999693870544%, loss=0.1837976182103157, time=2087.312808275223s\n",
      "Train epoch 17: top1=0.3308999836444855%, top2=0.4443499743938446%, top3=0.5135399699211121%, top4=0.5627899765968323%, top5=0.6010400056838989%, loss=0.17978725282073021, time=2089.3834545612335s\n",
      "Train epoch 18: top1=0.3432599902153015%, top2=0.4578299820423126%, top3=0.5273399949073792%, top4=0.5764999985694885%, top5=0.6137499809265137%, loss=0.17576532312989235, time=2087.822431564331s\n",
      "Train epoch 19: top1=0.35196998715400696%, top2=0.4694799780845642%, top3=0.5401399731636047%, top4=0.5886200070381165%, top5=0.6263499855995178%, loss=0.17217018787384034, time=2084.5847425460815s\n",
      "Train epoch 20: top1=0.36365997791290283%, top2=0.4810199737548828%, top3=0.5522899627685547%, top4=0.6013099551200867%, top5=0.6381199955940247%, loss=0.1681665761244297, time=2087.330841779709s\n",
      "Train epoch 21: top1=0.3754200041294098%, top2=0.49487999081611633%, top3=0.5641099810600281%, top4=0.6134999990463257%, top5=0.6511200070381165%, loss=0.1644804250562191, time=2089.054652929306s\n",
      "Train epoch 22: top1=0.3823999762535095%, top2=0.5034899711608887%, top3=0.5748199820518494%, top4=0.6235299706459045%, top5=0.6606400012969971%, loss=0.16148763288259507, time=2088.6889357566833s\n",
      "Train epoch 23: top1=0.39374998211860657%, top2=0.5139699578285217%, top3=0.5830599665641785%, top4=0.6323699951171875%, top5=0.6685799956321716%, loss=0.15831974939644336, time=2088.9505212306976s\n",
      "Train epoch 24: top1=0.40362998843193054%, top2=0.5244699716567993%, top3=0.5949599742889404%, top4=0.6434499621391296%, top5=0.6800699830055237%, loss=0.15486600335121153, time=2087.0894236564636s\n",
      "Train epoch 25: top1=0.4136299788951874%, top2=0.5345999598503113%, top3=0.605459988117218%, top4=0.6535599827766418%, top5=0.6900299787521362%, loss=0.15189935443758965, time=2096.079543352127s\n",
      "Train epoch 26: top1=0.4223899841308594%, top2=0.5453299880027771%, top3=0.6161999702453613%, top4=0.6641499996185303%, top5=0.7005400061607361%, loss=0.14857264832556247, time=2111.549813747406s\n",
      "Train epoch 27: top1=0.43080997467041016%, top2=0.554889976978302%, top3=0.6255699992179871%, top4=0.6726700067520142%, top5=0.7083399891853333%, loss=0.14595771338999272, time=2109.907327413559s\n",
      "Train epoch 28: top1=0.44071999192237854%, top2=0.566100001335144%, top3=0.6372599601745605%, top4=0.684149980545044%, top5=0.7192699909210205%, loss=0.14258959836542606, time=2109.38844871521s\n",
      "Train epoch 29: top1=0.44971999526023865%, top2=0.5763899683952332%, top3=0.6446200013160706%, top4=0.6907100081443787%, top5=0.725600004196167%, loss=0.13956691717982292, time=2109.8784165382385s\n",
      "Train epoch 30: top1=0.4563699960708618%, top2=0.5837799906730652%, top3=0.6546599864959717%, top4=0.7025299668312073%, top5=0.7368999719619751%, loss=0.1365710990858078, time=2108.986063718796s\n",
      "Train epoch 31: top1=0.4672199785709381%, top2=0.5950199961662292%, top3=0.6644299626350403%, top4=0.7103999853134155%, top5=0.744189977645874%, loss=0.13367001675605775, time=2109.936327934265s\n",
      "Train epoch 32: top1=0.4751800000667572%, top2=0.6031399965286255%, top3=0.6732999682426453%, top4=0.719219982624054%, top5=0.7529900074005127%, loss=0.1309129737138748, time=2110.5764865875244s\n",
      "Train epoch 33: top1=0.4846999943256378%, top2=0.6131500005722046%, top3=0.6839199662208557%, top4=0.7296199798583984%, top5=0.7627300024032593%, loss=0.12794877425968648, time=2110.1622397899628s\n",
      "Train epoch 34: top1=0.49334999918937683%, top2=0.6225599646568298%, top3=0.6916499733924866%, top4=0.7366399765014648%, top5=0.7691299915313721%, loss=0.12540273970603943, time=2106.574485063553s\n",
      "Train epoch 35: top1=0.5019199848175049%, top2=0.6317200064659119%, top3=0.7005299925804138%, top4=0.7468199729919434%, top5=0.7788800001144409%, loss=0.12250487145841121, time=2104.2957718372345s\n",
      "Train epoch 36: top1=0.5086299777030945%, top2=0.6407899856567383%, top3=0.7099800109863281%, top4=0.7546600103378296%, top5=0.7865599989891052%, loss=0.11993562625944615, time=2108.608437538147s\n",
      "Train epoch 37: top1=0.5187000036239624%, top2=0.6487399935722351%, top3=0.7173100113868713%, top4=0.7620099782943726%, top5=0.7941399812698364%, loss=0.11732988803327084, time=2107.9596271514893s\n",
      "Train epoch 38: top1=0.5253999829292297%, top2=0.6583200097084045%, top3=0.7268099784851074%, top4=0.7712900042533875%, top5=0.801289975643158%, loss=0.11454899571150541, time=2109.3296897411346s\n",
      "Train epoch 39: top1=0.5349900126457214%, top2=0.6647799611091614%, top3=0.7326499819755554%, top4=0.776199996471405%, top5=0.8065699934959412%, loss=0.11232388001680374, time=2105.211741924286s\n",
      "Train epoch 40: top1=0.5414499640464783%, top2=0.674519956111908%, top3=0.7412199974060059%, top4=0.7844299674034119%, top5=0.8140499591827393%, loss=0.1097953015768528, time=2104.206737279892s\n",
      "Train epoch 41: top1=0.5514199733734131%, top2=0.6824199557304382%, top3=0.7496599555015564%, top4=0.791949987411499%, top5=0.8213299512863159%, loss=0.10731006212547421, time=2107.5007371902466s\n",
      "Train epoch 42: top1=0.5582999587059021%, top2=0.6902899742126465%, top3=0.7567099928855896%, top4=0.7988499999046326%, top5=0.828249990940094%, loss=0.10494793818593025, time=2108.3469574451447s\n",
      "Train epoch 43: top1=0.5662299990653992%, top2=0.6994099617004395%, top3=0.7651000022888184%, top4=0.8067899942398071%, top5=0.835099995136261%, loss=0.10243269961982965, time=2108.3869915008545s\n",
      "Train epoch 44: top1=0.5736100077629089%, top2=0.7069999575614929%, top3=0.7722100019454956%, top4=0.8120200037956238%, top5=0.8403499722480774%, loss=0.10025827570259571, time=2105.381015062332s\n",
      "Train epoch 45: top1=0.5819199681282043%, top2=0.7146300077438354%, top3=0.7804799675941467%, top4=0.8208799958229065%, top5=0.8480899930000305%, loss=0.09784381466671825, time=2105.469699859619s\n",
      "Train epoch 46: top1=0.5870999693870544%, top2=0.7189899682998657%, top3=0.7848599553108215%, top4=0.823639988899231%, top5=0.8512199521064758%, loss=0.09629391669631004, time=2106.405618906021s\n",
      "Train epoch 47: top1=0.5957799553871155%, top2=0.7279999852180481%, top3=0.7916899919509888%, top4=0.8307600021362305%, top5=0.8571999669075012%, loss=0.09401408318385482, time=2106.9195425510406s\n",
      "Train epoch 48: top1=0.6018099784851074%, top2=0.7346099615097046%, top3=0.7973999977111816%, top4=0.8355499505996704%, top5=0.8623299598693848%, loss=0.09191307561844587, time=2105.9802725315094s\n",
      "Train epoch 49: top1=0.609529972076416%, top2=0.7411499619483948%, top3=0.803849995136261%, top4=0.8421199917793274%, top5=0.8676599860191345%, loss=0.09018349591940641, time=2107.685579061508s\n",
      "Train epoch 50: top1=0.615839958190918%, top2=0.7482799887657166%, top3=0.8103699684143066%, top4=0.84757000207901%, top5=0.872759997844696%, loss=0.08789931777492166, time=2106.902270793915s\n",
      "Train epoch 51: top1=0.6210199594497681%, top2=0.75409996509552%, top3=0.8154999613761902%, top4=0.853190004825592%, top5=0.8780199885368347%, loss=0.08609653338000178, time=2104.306448698044s\n",
      "Train epoch 52: top1=0.6269299983978271%, top2=0.7603099942207336%, top3=0.820930004119873%, top4=0.8576399683952332%, top5=0.8820799589157104%, loss=0.08429652892708778, time=2104.796110868454s\n",
      "Train epoch 53: top1=0.6328200101852417%, top2=0.7661899924278259%, top3=0.8276000022888184%, top4=0.8641299605369568%, top5=0.8883199691772461%, loss=0.08241084881410003, time=2104.372393846512s\n",
      "Train epoch 54: top1=0.6394999623298645%, top2=0.7715599536895752%, top3=0.832319974899292%, top4=0.8680599927902222%, top5=0.8918299674987793%, loss=0.08089263841018081, time=2048.0934903621674s\n",
      "Train epoch 55: top1=0.6451199650764465%, top2=0.7767699956893921%, top3=0.8360399603843689%, top4=0.8718799948692322%, top5=0.8946499824523926%, loss=0.07939874183237552, time=2024.975857257843s\n",
      "Train epoch 56: top1=0.6512399911880493%, top2=0.7813799977302551%, top3=0.8414499759674072%, top4=0.876259982585907%, top5=0.8991899490356445%, loss=0.07757541147440672, time=2022.2985653877258s\n",
      "Train epoch 57: top1=0.657509982585907%, top2=0.7877500057220459%, top3=0.8480100035667419%, top4=0.8816499710083008%, top5=0.904289960861206%, loss=0.07574873407900333, time=2024.8412580490112s\n",
      "Train epoch 58: top1=0.6628199815750122%, top2=0.7922700047492981%, top3=0.8497699499130249%, top4=0.8839499950408936%, top5=0.9065499901771545%, loss=0.07459330330908298, time=2030.8312196731567s\n",
      "Train epoch 59: top1=0.6706299781799316%, top2=0.7985699772834778%, top3=0.854919970035553%, top4=0.8883299827575684%, top5=0.9105599522590637%, loss=0.07283685043558479, time=2087.3075518608093s\n",
      "Train epoch 60: top1=0.6720799803733826%, top2=0.8018999695777893%, top3=0.858489990234375%, top4=0.8907600045204163%, top5=0.9131199717521667%, loss=0.0718947123517096, time=2083.9494087696075s\n",
      "Train epoch 61: top1=0.6772399544715881%, top2=0.806879997253418%, top3=0.8635199666023254%, top4=0.895389974117279%, top5=0.9160199761390686%, loss=0.07047741133749486, time=2087.5483717918396s\n",
      "Train epoch 62: top1=0.6825900077819824%, top2=0.8106899857521057%, top3=0.8668899536132812%, top4=0.8992899656295776%, top5=0.919189989566803%, loss=0.06900653736963869, time=2084.194162130356s\n",
      "Train epoch 63: top1=0.6870399713516235%, top2=0.8143199682235718%, top3=0.8691999912261963%, top4=0.9015399813652039%, top5=0.9222999811172485%, loss=0.06800117944628001, time=2084.566597223282s\n",
      "Train epoch 64: top1=0.6910799741744995%, top2=0.8193199634552002%, top3=0.8743599653244019%, top4=0.9051100015640259%, top5=0.9246699810028076%, loss=0.06664911009013653, time=2088.2999787330627s\n",
      "Train epoch 65: top1=0.6978999972343445%, top2=0.8227399587631226%, top3=0.8771199584007263%, top4=0.907539963722229%, top5=0.9270399808883667%, loss=0.06536285780265927, time=2087.9578564167023s\n",
      "Train epoch 66: top1=0.7003200054168701%, top2=0.8266599774360657%, top3=0.8805599808692932%, top4=0.9118099808692932%, top5=0.9301499724388123%, loss=0.0642849213975668, time=2088.6499037742615s\n",
      "Train epoch 67: top1=0.7039099931716919%, top2=0.8305000066757202%, top3=0.883929967880249%, top4=0.9127099514007568%, top5=0.9312199950218201%, loss=0.06314045227199792, time=2042.721939086914s\n",
      "Train epoch 68: top1=0.7080999612808228%, top2=0.8332699537277222%, top3=0.8857399821281433%, top4=0.9154999852180481%, top5=0.9344399571418762%, loss=0.062201786298453805, time=2037.525694847107s\n",
      "Train epoch 69: top1=0.7118899822235107%, top2=0.8368200063705444%, top3=0.8897299766540527%, top4=0.9181399941444397%, top5=0.9366999864578247%, loss=0.06125699442431331, time=2038.2180433273315s\n",
      "Train epoch 70: top1=0.7172399759292603%, top2=0.8402799963951111%, top3=0.8916800022125244%, top4=0.9203099608421326%, top5=0.9381799697875977%, loss=0.06040214445196092, time=2078.2313566207886s\n",
      "Train epoch 71: top1=0.7202199697494507%, top2=0.8422499895095825%, top3=0.8949199914932251%, top4=0.9227399826049805%, top5=0.9404599666595459%, loss=0.05946127482108772, time=2102.773900985718s\n",
      "Train epoch 72: top1=0.7224000096321106%, top2=0.8472399711608887%, top3=0.8975799679756165%, top4=0.9243099689483643%, top5=0.9414499998092651%, loss=0.058555980889648196, time=2104.906324863434s\n",
      "Train epoch 73: top1=0.7291399836540222%, top2=0.8498199582099915%, top3=0.9001599550247192%, top4=0.9270599484443665%, top5=0.9435699582099915%, loss=0.05715029853150248, time=2104.018521785736s\n",
      "Train epoch 74: top1=0.73117995262146%, top2=0.8528800010681152%, top3=0.9030599594116211%, top4=0.9294899702072144%, top5=0.9464899897575378%, loss=0.05653483512803912, time=2105.260530948639s\n",
      "Train epoch 75: top1=0.7350499629974365%, top2=0.8557899594306946%, top3=0.9054999947547913%, top4=0.9320600032806396%, top5=0.948479950428009%, loss=0.05556844758585095, time=2104.104088783264s\n",
      "Train epoch 76: top1=0.738319993019104%, top2=0.8597099781036377%, top3=0.9078399538993835%, top4=0.9339399933815002%, top5=0.9496099948883057%, loss=0.05468678731970489, time=2104.4847643375397s\n",
      "Train epoch 77: top1=0.7405200004577637%, top2=0.8611699938774109%, top3=0.9101999998092651%, top4=0.9363099932670593%, top5=0.9515900015830994%, loss=0.05400431739144027, time=2104.909543275833s\n",
      "Train epoch 78: top1=0.7443000078201294%, top2=0.8642500042915344%, top3=0.9121800065040588%, top4=0.9374199509620667%, top5=0.9523800015449524%, loss=0.05323379670701921, time=2104.424625635147s\n",
      "Train epoch 79: top1=0.7476399540901184%, top2=0.8657099604606628%, top3=0.913129985332489%, top4=0.9376599788665771%, top5=0.95346999168396%, loss=0.0524956694547832, time=2102.4450936317444s\n",
      "Train epoch 80: top1=0.7490800023078918%, top2=0.8684599995613098%, top3=0.9151999950408936%, top4=0.9401800036430359%, top5=0.9548699855804443%, loss=0.05181544233512134, time=2102.6760239601135s\n",
      "Train epoch 81: top1=0.753059983253479%, top2=0.8707499504089355%, top3=0.9179199934005737%, top4=0.9424899816513062%, top5=0.9566699862480164%, loss=0.05115741480786353, time=2104.063131570816s\n",
      "Train epoch 82: top1=0.7548799514770508%, top2=0.8723099827766418%, top3=0.9183299541473389%, top4=0.9436299800872803%, top5=0.9582200050354004%, loss=0.05063689898211509, time=2101.6820199489594s\n",
      "Train epoch 83: top1=0.7591300010681152%, top2=0.8761399984359741%, top3=0.920229971408844%, top4=0.9446899890899658%, top5=0.9587799906730652%, loss=0.04969543795578182, time=2103.38720202446s\n",
      "Train epoch 84: top1=0.7624899744987488%, top2=0.8788299560546875%, top3=0.9240700006484985%, top4=0.9467299580574036%, top5=0.9610299468040466%, loss=0.048770716232266274, time=2103.0407412052155s\n",
      "Train epoch 85: top1=0.7640500068664551%, top2=0.8810299634933472%, top3=0.925529956817627%, top4=0.9483799934387207%, top5=0.9619799852371216%, loss=0.04816737629648298, time=2100.014773607254s\n",
      "Train epoch 86: top1=0.7678700089454651%, top2=0.8823599815368652%, top3=0.925879955291748%, top4=0.9491499662399292%, top5=0.9623299837112427%, loss=0.04776338645670563, time=2098.6651549339294s\n",
      "Train epoch 87: top1=0.7716400027275085%, top2=0.8842999935150146%, top3=0.9279699921607971%, top4=0.9498899579048157%, top5=0.963699996471405%, loss=0.046919843935575335, time=2094.676144361496s\n",
      "Train epoch 88: top1=0.7734599709510803%, top2=0.8866999745368958%, top3=0.9290099740028381%, top4=0.9505899548530579%, top5=0.9634499549865723%, loss=0.04668783585678786, time=2095.4053962230682s\n",
      "Train epoch 89: top1=0.7763699889183044%, top2=0.8882299661636353%, top3=0.9310199618339539%, top4=0.9518799781799316%, top5=0.9651100039482117%, loss=0.04590039169920608, time=2095.2821860313416s\n",
      "Train epoch 90: top1=0.7787299752235413%, top2=0.8904799818992615%, top3=0.9318299889564514%, top4=0.9534199833869934%, top5=0.9658499956130981%, loss=0.04520039220701903, time=2096.065985918045s\n",
      "Train epoch 91: top1=0.779670000076294%, top2=0.8913699984550476%, top3=0.9327399730682373%, top4=0.954479992389679%, top5=0.967519998550415%, loss=0.04483031625684351, time=2095.283844470978s\n",
      "Train epoch 92: top1=0.7839699983596802%, top2=0.8949399590492249%, top3=0.9354199767112732%, top4=0.9561299681663513%, top5=0.9684900045394897%, loss=0.04395578148405999, time=2097.8713204860687s\n",
      "Train epoch 93: top1=0.7831400036811829%, top2=0.8953700065612793%, top3=0.9362499713897705%, top4=0.9568299651145935%, top5=0.9688699841499329%, loss=0.04390775315931067, time=2098.6280133724213s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     report_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# report_test = test_epoch(test_loader, epoch)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     report \u001b[38;5;241m=\u001b[39m report_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#+ report_test + '\\n\\n'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m topk_accuracy(outputs, targets, topk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/optim/adam.py:524\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[0;32m--> 524\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Define train and test functions (use examples)\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # print(f'batch{i} done!')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = f'Train epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_train)\n",
    "\n",
    "    return report_train\n",
    "\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
