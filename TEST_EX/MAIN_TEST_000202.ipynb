{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "# convolution patch embedding\n",
    "# from Tensorized_components.patch_embedding  import Patch_Embedding        \n",
    "from Tensorized_components.tcl_patch_embedding  import  PatchEmbedding  as  Patch_Embedding      \n",
    "from Tensorized_components.w_msa_w_o_b_sign  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b_sign import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL  import TCL  as TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.mnist_loaders_count import get_mnist_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing 'Block 1' in your Swin Transformer.\n",
    "    This captures the sequence of:\n",
    "        (1) Window MSA + residual\n",
    "        (2) TCL + residual\n",
    "        (3) Shifted Window MSA + residual\n",
    "        (4) TCL + residual\n",
    "    but only for the first block’s hyperparameters and submodules.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,tcl2,tcl3,tcl4, embed_shape, dropout=0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        # Typically each sub-layer has its own LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We pass in pre-built modules (WindowMSA, ShiftedWindowMSA, TCL)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "    def forward(self, x):\n",
    "        # ----- First Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- Shifted Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4,  embed_shape=(4,4,6), dropout=0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "        # LN layers\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # Shifted Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock3(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,  tcl2 , tcl3 , tcl4,   embed_shape=(4,4,12), dropout=0):\n",
    "        super(SwinBlock3, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock4(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4 ,  embed_shape=(4,4,24), dropout=0):\n",
    "        super(SwinBlock4, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "# tcl patch embedding \n",
    "\n",
    "\n",
    "# TODO : change batch size and device to cuda\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            input_size=(16,3,224,224),\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_shape,\n",
    "            bias=bias,\n",
    "            device=\"cpu\",\n",
    "            ignore_modes = (0,1,2)\n",
    "        )\n",
    "# convolution \n",
    "        # self.patch_embedding = Patch_Embedding(\n",
    "        #     img_size=img_size,\n",
    "        #     patch_size=patch_size,\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_shape=embed_shape,\n",
    "        #     bias=bias\n",
    "        # )\n",
    "\n",
    "        # -------------------------------- block 1 --------------------------\n",
    "\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_2 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_3 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_4 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl1=self.tcl_1,\n",
    "                tcl2 = self.tcl_1_2,\n",
    "                tcl3 = self.tcl_1_3,\n",
    "                tcl4 = self.tcl_1_4,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- block 2 --------------------------\n",
    "\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(4,4,24),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.tcl_2_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2_3 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.tcl_2_4 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # We repeat Block2 two times\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                tcl1=self.tcl_2,\n",
    "                tcl2 = self.tcl_2_2,\n",
    "                tcl3 = self.tcl_2_3,\n",
    "                tcl4 = self.tcl_2_4,\n",
    "                embed_shape=(4,4,24),  \n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # # -------------------------------- block 3 --------------------------\n",
    "\n",
    "        self.patch_merging_2 = TensorizedPatchMerging(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            in_embed_shape=(4,4,24),\n",
    "            out_embed_shape=(4,4,48),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.w_msa_3 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_3 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_3_2 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_4 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Repeat Block3 6 times\n",
    "        self.block3_list = nn.ModuleList([\n",
    "            SwinBlock3(\n",
    "                w_msa=self.w_msa_3,\n",
    "                sw_msa=self.sw_msa_3,\n",
    "                tcl1=self.tcl_3,\n",
    "                tcl2=self.tcl_3_2,\n",
    "                tcl3=self.tcl_3_3,\n",
    "                tcl4=self.tcl_3_4,\n",
    "                embed_shape=(4,4,48),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(18)\n",
    "        ])\n",
    "\n",
    "        # # # -------------------------------- block 4 --------------------------\n",
    "\n",
    "        self.patch_merging_3 = TensorizedPatchMerging(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            in_embed_shape=(4,4,48),\n",
    "            out_embed_shape=(4,4,96),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_4 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_4 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4_2 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_3 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block4_list = nn.ModuleList([\n",
    "            SwinBlock4(\n",
    "                w_msa=self.w_msa_4,\n",
    "                sw_msa=self.sw_msa_4,\n",
    "                tcl1=self.tcl_4,\n",
    "                tcl2=self.tcl_4_2,\n",
    "                tcl3=self.tcl_4_3,\n",
    "                tcl4=self.tcl_4_4,\n",
    "                embed_shape=(4,4,96),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- classifier --------------------------\n",
    "\n",
    "    \n",
    "\n",
    "        self.classifier = TRL(input_size=(16,4,4,96),\n",
    "                            output=(200,),\n",
    "                            rank=(4,4,96,200),\n",
    "                            ignore_modes=(0,),\n",
    "                            bias=bias,\n",
    "                            device=self.device) \n",
    "        \n",
    "\n",
    "        # positoin embedding\n",
    "\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1,\n",
    "                        56,\n",
    "                        56,\n",
    "                        4,\n",
    "                        4,\n",
    "                        12,\n",
    "                        device = self.device\n",
    "                        ), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for i, blk in enumerate(self.block1_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block2_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_2(x)\n",
    "\n",
    "        for i, blk in enumerate(self.block3_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_3(x)\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block4_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cuda\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [01:30<00:00, 109kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 129kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:14<00:00, 113kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 27.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID00090'\n",
    "batch_size = 16\n",
    "n_epoch = 400\n",
    "image_size = 224\n",
    "train_size = \"default\"\n",
    "\n",
    "model = SwinTransformer(img_size=224,patch_size=4,in_chans=3,embed_shape=(4,4,12),bias=True,device=device).to(device)\n",
    "\n",
    "# Set up the transforms and train/test loaders\n",
    "transform_train = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            RandAugment(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            RandomErasing(p=0.25)\n",
    "        ])\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "train_loader, _ = get_mnist_dataloaders('../datasets', transform_train, transform_test, batch_size, image_size, train_size, repeat_count=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 1700556 parameters\n",
      "Training for 400 epochs\n",
      "\n",
      "Train epoch 1: top1=0.5164166688919067%, top2=0.6632944345474243%, top3=0.7473000288009644%, top4=0.804722249507904%, top5=0.850849986076355%, loss=0.0831895475069268, time=3821.087176799774s\n",
      "Train epoch 2: top1=0.8290444612503052%, top2=0.9291666746139526%, top3=0.9640499949455261%, top4=0.9803110957145691%, top5=0.9891444444656372%, loss=0.031709318662910824, time=3826.5717062950134s\n",
      "Train epoch 3: top1=0.8797944784164429%, top2=0.9546833634376526%, top3=0.978172242641449%, top4=0.9887166619300842%, top5=0.9939777851104736%, loss=0.022551028553973364, time=3827.565776824951s\n",
      "Train epoch 4: top1=0.9044555425643921%, top2=0.9653333425521851%, top3=0.983738899230957%, top4=0.9920111298561096%, top5=0.995888888835907%, loss=0.018043731030529468, time=3829.683053970337s\n",
      "Train epoch 5: top1=0.9198333621025085%, top2=0.9723166823387146%, top3=0.9870389103889465%, top4=0.9936500191688538%, top5=0.9968888759613037%, loss=0.015327478774201073, time=3829.5620260238647s\n",
      "Train epoch 6: top1=0.9291833639144897%, top2=0.9764778017997742%, top3=0.9894333481788635%, top4=0.9948055744171143%, top5=0.9973055720329285%, loss=0.013462027145601395, time=3827.8701305389404s\n",
      "Train epoch 7: top1=0.9361555576324463%, top2=0.9798166751861572%, top3=0.9912833571434021%, top4=0.9958500266075134%, top5=0.9978833198547363%, loss=0.01216850533810179, time=3827.4831643104553s\n",
      "Train epoch 8: top1=0.9416611194610596%, top2=0.9814167022705078%, top3=0.9921111464500427%, top4=0.9962666630744934%, top5=0.9981777667999268%, loss=0.011126129589342924, time=3819.264996290207s\n",
      "Train epoch 9: top1=0.9460499882698059%, top2=0.9831611514091492%, top3=0.9926555752754211%, top4=0.9966555833816528%, top5=0.9984500408172607%, loss=0.010347735254849976, time=3825.248281955719s\n",
      "Train epoch 10: top1=0.9498833417892456%, top2=0.9849555492401123%, top3=0.9934999942779541%, top4=0.9969000220298767%, top5=0.9985611438751221%, loss=0.009623486285711504, time=3825.00239610672s\n",
      "Train epoch 11: top1=0.951449990272522%, top2=0.9858778119087219%, top3=0.9940277934074402%, top4=0.9973000288009644%, top5=0.9985944628715515%, loss=0.009301841586898323, time=3826.9686827659607s\n",
      "Train epoch 12: top1=0.9539777636528015%, top2=0.9864166975021362%, top3=0.9942666888237%, top4=0.9972388744354248%, top5=0.9986777901649475%, loss=0.008848309687487906, time=3823.9761760234833s\n",
      "Train epoch 13: top1=0.9557722210884094%, top2=0.9874666929244995%, top3=0.9946610927581787%, top4=0.9976555705070496%, top5=0.9988722205162048%, loss=0.008393015513385277, time=3826.223655462265s\n",
      "Train epoch 14: top1=0.9572166800498962%, top2=0.988027811050415%, top3=0.9948500394821167%, top4=0.9976444840431213%, top5=0.9989166855812073%, loss=0.00820957706833833, time=3830.940833091736s\n",
      "Train epoch 15: top1=0.9585111141204834%, top2=0.9886888861656189%, top3=0.9953222274780273%, top4=0.997938871383667%, top5=0.9989611506462097%, loss=0.007900117061771197, time=3828.6829512119293s\n",
      "Train epoch 16: top1=0.9592666625976562%, top2=0.988788902759552%, top3=0.9955500364303589%, top4=0.9979944825172424%, top5=0.998977780342102%, loss=0.007792949826040098, time=3822.2177085876465s\n",
      "Train epoch 17: top1=0.9602000117301941%, top2=0.9891611337661743%, top3=0.9957722425460815%, top4=0.9980611205101013%, top5=0.9990833401679993%, loss=0.007589669693754755, time=3825.5631482601166s\n",
      "Train epoch 18: top1=0.96147221326828%, top2=0.9895222187042236%, top3=0.9958333373069763%, top4=0.998033344745636%, top5=0.999061107635498%, loss=0.007360408751632045, time=3828.5368206501007s\n",
      "Train epoch 19: top1=0.9623777866363525%, top2=0.9901278018951416%, top3=0.9959388971328735%, top4=0.9981555938720703%, top5=0.9991778135299683%, loss=0.007197998421868983, time=3827.338925600052s\n",
      "Train epoch 20: top1=0.9622833728790283%, top2=0.9899222254753113%, top3=0.996066689491272%, top4=0.9981777667999268%, top5=0.9992055892944336%, loss=0.007168042534590151, time=3827.4300796985626s\n",
      "Train epoch 21: top1=0.9633722305297852%, top2=0.9906833171844482%, top3=0.9963111281394958%, top4=0.9983833432197571%, top5=0.99922776222229%, loss=0.00696317228266095, time=3833.1483478546143s\n",
      "Train epoch 22: top1=0.9632055759429932%, top2=0.9903610944747925%, top3=0.9961888790130615%, top4=0.998283326625824%, top5=0.9991666674613953%, loss=0.006933207285609812, time=3931.157454967499s\n",
      "Train epoch 23: top1=0.9647889137268066%, top2=0.9907833337783813%, top3=0.996138870716095%, top4=0.9983722567558289%, top5=0.9991999864578247%, loss=0.006770663281198075, time=3931.1018209457397s\n",
      "Train epoch 24: top1=0.965077817440033%, top2=0.9910277724266052%, top3=0.9964166879653931%, top4=0.9984722137451172%, top5=0.9992444515228271%, loss=0.0066593472910851485, time=3931.428293943405s\n",
      "Train epoch 25: top1=0.9654333591461182%, top2=0.9910666942596436%, top3=0.9964889287948608%, top4=0.998461127281189%, top5=0.9992611408233643%, loss=0.006581605736229621, time=3929.169139623642s\n",
      "Train epoch 26: top1=0.965327799320221%, top2=0.9912333488464355%, top3=0.9964610934257507%, top4=0.998461127281189%, top5=0.99922776222229%, loss=0.006504604832852166, time=3929.2557418346405s\n",
      "Train epoch 27: top1=0.9662666916847229%, top2=0.9915333390235901%, top3=0.9967555403709412%, top4=0.9985944628715515%, top5=0.9994111061096191%, loss=0.00637858983804494, time=3930.335605621338s\n",
      "Train epoch 28: top1=0.9665777683258057%, top2=0.9916388988494873%, top3=0.9967055916786194%, top4=0.9985389113426208%, top5=0.9993111491203308%, loss=0.006301662280228498, time=3927.5415980815887s\n",
      "Train epoch 29: top1=0.9670166969299316%, top2=0.9916278123855591%, top3=0.9965944290161133%, top4=0.998544454574585%, top5=0.9992833733558655%, loss=0.006259983689733471, time=3924.2413890361786s\n",
      "Train epoch 30: top1=0.9672722220420837%, top2=0.9916444420814514%, top3=0.996916651725769%, top4=0.9986055493354797%, top5=0.9993389248847961%, loss=0.006256483909055724, time=3924.0469632148743s\n",
      "Train epoch 31: top1=0.9675722122192383%, top2=0.9918000102043152%, top3=0.9966278076171875%, top4=0.9985111355781555%, top5=0.9992777705192566%, loss=0.006193140482574644, time=3924.565470457077s\n",
      "Train epoch 32: top1=0.9675833582878113%, top2=0.99186110496521%, top3=0.9968777894973755%, top4=0.9986667037010193%, top5=0.999322235584259%, loss=0.006148061099871039, time=3923.1103959083557s\n",
      "Train epoch 33: top1=0.9687111377716064%, top2=0.9921389222145081%, top3=0.9968833327293396%, top4=0.9986444711685181%, top5=0.9994222521781921%, loss=0.005993634327642702, time=3923.9293706417084s\n",
      "Train epoch 34: top1=0.968927800655365%, top2=0.9923499822616577%, top3=0.9970999956130981%, top4=0.9987000226974487%, top5=0.9994388818740845%, loss=0.005899165605510648, time=3927.4589545726776s\n",
      "Train epoch 35: top1=0.9686055779457092%, top2=0.9924833178520203%, top3=0.9970055818557739%, top4=0.998627781867981%, top5=0.9993944764137268%, loss=0.005924577924436219, time=3929.1861617565155s\n",
      "Train epoch 36: top1=0.9686999917030334%, top2=0.9924389123916626%, top3=0.9968888759613037%, top4=0.998711109161377%, top5=0.9994388818740845%, loss=0.005937200893154507, time=3928.6655192375183s\n",
      "Train epoch 37: top1=0.9686722159385681%, top2=0.9925777912139893%, top3=0.9970166683197021%, top4=0.9987388849258423%, top5=0.9993833303451538%, loss=0.005878538853216676, time=3930.8761286735535s\n",
      "Train epoch 38: top1=0.9698500037193298%, top2=0.9927555918693542%, top3=0.9974666833877563%, top4=0.9987888932228088%, top5=0.9993833303451538%, loss=0.005755042354944498, time=3928.4222621917725s\n",
      "Train epoch 39: top1=0.969438910484314%, top2=0.9925166964530945%, top3=0.9971666932106018%, top4=0.9986833333969116%, top5=0.9993888735771179%, loss=0.0059063999249288224, time=3926.3815999031067s\n",
      "Train epoch 40: top1=0.9698055386543274%, top2=0.992566704750061%, top3=0.997094452381134%, top4=0.9987500309944153%, top5=0.9994111061096191%, loss=0.0057931928763777655, time=3929.426413536072s\n",
      "Train epoch 41: top1=0.9696222543716431%, top2=0.9924277663230896%, top3=0.9970777630805969%, top4=0.9988389015197754%, top5=0.9994778037071228%, loss=0.005723969376627418, time=3916.734636068344s\n",
      "Train epoch 42: top1=0.9702000021934509%, top2=0.9928500056266785%, top3=0.997272253036499%, top4=0.9988555908203125%, top5=0.9994111061096191%, loss=0.005657656948233327, time=3872.103286743164s\n",
      "Train epoch 43: top1=0.9705333709716797%, top2=0.9929277896881104%, top3=0.9972888827323914%, top4=0.9988611340522766%, top5=0.9994333386421204%, loss=0.005610460072173855, time=3855.7663440704346s\n",
      "Train epoch 44: top1=0.9705555438995361%, top2=0.9928389191627502%, top3=0.9972666501998901%, top4=0.9988166689872742%, top5=0.999488890171051%, loss=0.00565737600197502, time=3852.058747768402s\n",
      "Train epoch 45: top1=0.97038334608078%, top2=0.9928444623947144%, top3=0.9973222613334656%, top4=0.9988722205162048%, top5=0.9994500279426575%, loss=0.0056473109976389775, time=3857.273748397827s\n",
      "Train epoch 46: top1=0.9709833264350891%, top2=0.9930333495140076%, top3=0.997344434261322%, top4=0.9988889098167419%, top5=0.9994388818740845%, loss=0.005547070657730112, time=3790.2315928936005s\n",
      "Train epoch 47: top1=0.9707444310188293%, top2=0.9929555654525757%, top3=0.9972167015075684%, top4=0.9987611174583435%, top5=0.9994444847106934%, loss=0.005556136671635967, time=3756.8591401576996s\n",
      "Train epoch 48: top1=0.9710666537284851%, top2=0.9929611086845398%, top3=0.9971444606781006%, top4=0.9987055659294128%, top5=0.9994944334030151%, loss=0.005457625702350378, time=3754.532832622528s\n",
      "Train epoch 49: top1=0.9711166620254517%, top2=0.9932888746261597%, top3=0.9974666833877563%, top4=0.9989500045776367%, top5=0.9995222091674805%, loss=0.005449734865167455, time=3756.292805671692s\n",
      "Train epoch 50: top1=0.9715889096260071%, top2=0.993066668510437%, top3=0.9974777698516846%, top4=0.9988278150558472%, top5=0.9994444847106934%, loss=0.005493756301064532, time=3758.548025369644s\n",
      "Train epoch 51: top1=0.971311092376709%, top2=0.9931944608688354%, top3=0.9973333477973938%, top4=0.9988333582878113%, top5=0.9994500279426575%, loss=0.005430593614016086, time=3759.928045272827s\n",
      "Train epoch 52: top1=0.9716444611549377%, top2=0.9933111071586609%, top3=0.9975277781486511%, top4=0.9989222288131714%, top5=0.9994000196456909%, loss=0.005416641979822018, time=3760.1348991394043s\n",
      "Train epoch 53: top1=0.9710333347320557%, top2=0.9931277632713318%, top3=0.9974944591522217%, top4=0.9990167021751404%, top5=0.9995555877685547%, loss=0.00540746890993689, time=3757.9684455394745s\n",
      "Train epoch 54: top1=0.9714611172676086%, top2=0.9933388829231262%, top3=0.9974166750907898%, top4=0.998794436454773%, top5=0.999405562877655%, loss=0.005426627896371317, time=3757.2721519470215s\n",
      "Train epoch 55: top1=0.9722333550453186%, top2=0.9936666488647461%, top3=0.9974944591522217%, top4=0.998877763748169%, top5=0.9994833469390869%, loss=0.005288876781357087, time=3764.58021736145s\n",
      "Train epoch 56: top1=0.9721388816833496%, top2=0.9931222200393677%, top3=0.9974555373191833%, top4=0.9988833665847778%, top5=0.9994833469390869%, loss=0.00534942933874313, time=3758.546161174774s\n",
      "Train epoch 57: top1=0.9721722602844238%, top2=0.9936666488647461%, top3=0.9975055456161499%, top4=0.9988999962806702%, top5=0.9994666576385498%, loss=0.005318317214313449, time=3754.679327249527s\n",
      "Train epoch 58: top1=0.9717833399772644%, top2=0.9937111139297485%, top3=0.9975666999816895%, top4=0.9989389181137085%, top5=0.9995499849319458%, loss=0.005278155460579491, time=3757.4855716228485s\n",
      "Train epoch 59: top1=0.9726222157478333%, top2=0.9936777949333191%, top3=0.9976277947425842%, top4=0.9990388751029968%, top5=0.9995444416999817%, loss=0.005224420470973817, time=3756.853404045105s\n",
      "Train epoch 60: top1=0.9725500345230103%, top2=0.9937333464622498%, top3=0.9977499842643738%, top4=0.999072253704071%, top5=0.9995166659355164%, loss=0.005195122353516353, time=3756.4815485477448s\n",
      "Train epoch 61: top1=0.9731611013412476%, top2=0.9936555624008179%, top3=0.9978222250938416%, top4=0.9990777969360352%, top5=0.9995333552360535%, loss=0.005110728532989742, time=3756.123583316803s\n",
      "Train epoch 62: top1=0.9718388915061951%, top2=0.9937166571617126%, top3=0.9978111386299133%, top4=0.9989833235740662%, top5=0.9995666742324829%, loss=0.005249200827667119, time=3753.9562442302704s\n",
      "Train epoch 63: top1=0.972361147403717%, top2=0.9936555624008179%, top3=0.9975444674491882%, top4=0.9989611506462097%, top5=0.9995111227035522%, loss=0.0052380858673161486, time=3753.865965127945s\n",
      "Train epoch 64: top1=0.9726889133453369%, top2=0.9936500191688538%, top3=0.9976333379745483%, top4=0.9989611506462097%, top5=0.999572217464447%, loss=0.005192844164383645, time=3755.0052349567413s\n",
      "Train epoch 65: top1=0.9725722074508667%, top2=0.9939055442810059%, top3=0.9976500272750854%, top4=0.998977780342102%, top5=0.9995333552360535%, loss=0.005178845571279746, time=3755.4659011363983s\n",
      "Train epoch 66: top1=0.9730333685874939%, top2=0.9939166903495789%, top3=0.9979110956192017%, top4=0.9990333318710327%, top5=0.9995111227035522%, loss=0.005150683860825039, time=3756.471199274063s\n",
      "Train epoch 67: top1=0.9724500179290771%, top2=0.9937778115272522%, top3=0.9977111220359802%, top4=0.9990110993385315%, top5=0.9995111227035522%, loss=0.0050808055005846856, time=3755.1221420764923s\n",
      "Train epoch 68: top1=0.9731500148773193%, top2=0.9937222599983215%, top3=0.9976666569709778%, top4=0.9989555478096008%, top5=0.9995278120040894%, loss=0.0051345665705079275, time=3756.534905433655s\n",
      "Train epoch 69: top1=0.9728277921676636%, top2=0.9939000010490417%, top3=0.9977499842643738%, top4=0.998988926410675%, top5=0.9995499849319458%, loss=0.005114350472870325, time=3756.808027744293s\n",
      "Train epoch 70: top1=0.9735167026519775%, top2=0.9938666820526123%, top3=0.9975000023841858%, top4=0.9989389181137085%, top5=0.9994722604751587%, loss=0.005075950607267078, time=3754.44735121727s\n",
      "Train epoch 71: top1=0.9733611345291138%, top2=0.994005560874939%, top3=0.9978222250938416%, top4=0.9991333484649658%, top5=0.9995999932289124%, loss=0.0050201722240449925, time=3818.051404237747s\n",
      "Train epoch 72: top1=0.973811149597168%, top2=0.9938611388206482%, top3=0.9976666569709778%, top4=0.9990833401679993%, top5=0.9995777606964111%, loss=0.00505674735249505, time=3818.1670441627502s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     report_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# report_test = test_epoch(test_loader, epoch)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     report \u001b[38;5;241m=\u001b[39m report_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#+ report_test + '\\n\\n'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "# Define train and test functions (use examples)\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # print(f'batch{i} done!')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = f'Train epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_train)\n",
    "\n",
    "    return report_train\n",
    "\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
