{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00afb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# from Window_partition import WindowPartition\n",
    "from Tensorized_Layers.TCL import TCL as TCL_CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93acc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "# from Window_partition import WindowPartition\n",
    "from Tensorized_Layers.TCL_CHANGED import TCL_CHANGED\n",
    "\n",
    "\n",
    "class WindowPartition(nn.Module):\n",
    "    \"\"\"\n",
    "    Utility module for partitioning and reversing windows in a patch grid.\n",
    "\n",
    "    Input shape: (B, H, W, *embed_dims)\n",
    "    After partitioning with a given window_size, the tensor is reshaped into:\n",
    "        (B, H//window_size, W//window_size, window_size, window_size, *embed_dims)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int):\n",
    "        super(WindowPartition, self).__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Partition the input tensor into non-overlapping windows.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, H, W, *embed_dims).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Partitioned tensor with shape \n",
    "                (B, H//window_size, W//window_size, window_size, window_size, *embed_dims).\n",
    "        \"\"\"\n",
    "        B, H, W, *embed_dims = x.shape\n",
    "        ws = self.window_size\n",
    "        if H % ws != 0 or W % ws != 0:\n",
    "            raise ValueError(\n",
    "                f\"H and W must be divisible by window_size {ws}. Got H={H}, W={W}.\")\n",
    "        # Reshape to split H and W into windows.\n",
    "        x = x.view(B, H // ws, ws, W // ws, ws, *embed_dims)\n",
    "        # Permute to group the window blocks together.\n",
    "        windows = x.permute(0, 1, 3, 2, 4, *range(5, x.dim()))\n",
    "        return windows\n",
    "\n",
    "    def reverse(self, windows: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse the window partition to reconstruct the original tensor.\n",
    "\n",
    "        Args:\n",
    "            windows (torch.Tensor): Partitioned windows with shape \n",
    "                (B, H//window_size, W//window_size, window_size, window_size, *embed_dims).\n",
    "            H (int): Original height.\n",
    "            W (int): Original width.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed tensor of shape (B, H, W, *embed_dims).\n",
    "        \"\"\"\n",
    "        ws = self.window_size\n",
    "        B, num_h, num_w, ws1, ws2, *embed_dims = windows.shape\n",
    "        # Permute back to interleave the window dimensions.\n",
    "        x = windows.permute(\n",
    "            0, 1, 3, 2, 4, *range(5, windows.dim())).contiguous()\n",
    "        # Reshape to reconstruct the original feature map.\n",
    "        x = x.view(B, num_h * ws1, num_w * ws2, *embed_dims)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WindowMSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multi-Head Self-Attention (W-MSA) module.\n",
    "\n",
    "    This module partitions the input tensor into windows, computes tensorized Q, K, V\n",
    "    using TCL layers (which operate on each window), applies relative positional bias,\n",
    "    computes self-attention scores, and reconstructs the full feature map.\n",
    "\n",
    "    Args:\n",
    "        window_size (int): Spatial size of the window (e.g., 7).\n",
    "        embed_dims (tuple): Embedding dimensions for each patch (e.g., (4, 4, 3)).\n",
    "        rank_window (tuple): Output dimensions from TCL layers for each window \n",
    "                             (e.g., (4, 4, 3)). These should be divisible by the\n",
    "                             corresponding head factors.\n",
    "        head_factors (tuple): Factors to split the TCL output channels into heads.\n",
    "                              For example, (2, 2, 1) will yield 2*2*1 = 4 heads.\n",
    "        device (str): Device identifier (default 'cpu').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int, embed_dims: tuple, rank_window: tuple, head_factors: tuple, device='cpu'):\n",
    "        super(WindowMSA, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embed_dims = embed_dims      # e.g., (4, 4, 3)\n",
    "        self.rank_window = rank_window    # e.g., (4, 4, 3)\n",
    "        self.head_factors = head_factors  # e.g., (2, 2, 1)\n",
    "\n",
    "        self.device = device\n",
    "        # Number of heads is the product of the head factors.\n",
    "\n",
    "        self.scale = ((self.embed_dims[0] // self.head_factors[0]) *\n",
    "                      (self.embed_dims[1] // self.head_factors[1]) *\n",
    "                      (self.embed_dims[2] // self.head_factors[2])) ** (-0.5)\n",
    "\n",
    "        self.num_heads = 1\n",
    "        for h in head_factors:\n",
    "            self.num_heads *= h\n",
    "\n",
    "        # Input size for TCL layers for each window: (window_size, window_size, *embed_dims)\n",
    "        self.input_size_window = (32 , window_size, window_size) + embed_dims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"input size window\" , self.input_size_window)\n",
    "        \n",
    "\n",
    "        # Instantiate TCL layers for Q, K, and V.\n",
    "        self.tcl_q = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1 , 2), bias=True, device=self.device)\n",
    "        self.tcl_k = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1 , 2), bias=True, device=self.device)\n",
    "        self.tcl_v = TCL_CHANGED(input_size=self.input_size_window,\n",
    "                                 rank=rank_window, ignore_modes=(0, 1 , 2), bias=True, device=self.device)\n",
    "\n",
    "        # Create a learnable relative bias table.\n",
    "        self.rel_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size - 1) *\n",
    "                        (2 * window_size - 1), self.num_heads, device=self.device)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.rel_bias_table, std=0.02)\n",
    "\n",
    "        # Pre-compute relative position indices for a window.\n",
    "        coords_h = torch.arange(window_size, device=self.device)\n",
    "        coords_w = torch.arange(window_size, device=self.device)\n",
    "        coords = torch.stack(torch.meshgrid(\n",
    "            coords_h, coords_w, indexing='ij'))  # [2, ws, ws]\n",
    "        coords_flatten = torch.flatten(coords, 1)  # [2, ws*ws]\n",
    "        relative_coords = coords_flatten[:, :, None] - \\\n",
    "            coords_flatten[:, None, :]  # [2, ws*ws, ws*ws]\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0).contiguous()  # [ws*ws, ws*ws, 2]\n",
    "        relative_coords[:, :, 0] += window_size - 1\n",
    "        relative_coords[:, :, 1] += window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size - 1\n",
    "        # [ws*ws, ws*ws]\n",
    "        self.relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.window_partition = WindowPartition(window_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, *embed_shape = x.shape\n",
    "        ws = self.window_size\n",
    "\n",
    "\n",
    "\n",
    "        print(\"input size window\" , self.input_size_window)\n",
    "\n",
    "        # 1. Partition input into windows.\n",
    "        #    Shape: (B, H//ws, W//ws, ws, ws, *embed_dims)\n",
    "        x_windows = self.window_partition(x)\n",
    "\n",
    "        # print(x_windows.shape)\n",
    "        B, nH, nW, ws1, ws2, *_ = x_windows.shape  # ws1 == ws2 == window_size\n",
    "        num_windows = nH * nW\n",
    "\n",
    "        # 2. Reshape for window processing: (B*num_windows, ws, ws, *embed_dims)\n",
    "        x_windows = x_windows.reshape(B * num_windows, ws1, ws2, *embed_shape)\n",
    "\n",
    "        # 3. Compute Q, K, V using TCL layers.\n",
    "\n",
    "        # print(\"shape of x windows\" , x_windows.shape)\n",
    "\n",
    "\n",
    "        Q_windows = self.tcl_q(x_windows)\n",
    "        K_windows = self.tcl_k(x_windows)\n",
    "        V_windows = self.tcl_v(x_windows)\n",
    "\n",
    "        # 4. Reshape back to batch + windows.\n",
    "        Q_windows = Q_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "        K_windows = K_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "        V_windows = V_windows.view(B, nH, nW, ws1, ws2, *self.rank_window)\n",
    "\n",
    "        # 5. Split into multi-heads.\n",
    "        # Let head factors be h1, h2, h3 and TCL output rank be (r1, r2, r3).\n",
    "        h1, h2, h3 = self.head_factors  # e.g., (2, 2, 1)\n",
    "        r1, r2, r3 = self.rank_window     # e.g., (4, 4, 3)\n",
    "        # Rearrange such that TCL channels are split into head dimensions and remaining factors.\n",
    "        # New shape: (B, nH, nW, ws, ws, h1, h2, h3, x, y, z) where r1 = x*h1, etc.\n",
    "        q = rearrange(Q_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "        k = rearrange(K_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "        v = rearrange(V_windows, 'b m n i j (x a) (y d) (z e) -> b m n i j a d e x y z',\n",
    "                      a=h1, d=h2, e=h3)\n",
    "        \n",
    "\n",
    "        # print(q.shape)\n",
    "\n",
    "        # 6. Compute attention scores using einsum.\n",
    "        #    Here:\n",
    "        #      - b: batch\n",
    "        #      - m, n: window grid positions\n",
    "        #      - i, j: query window spatial positions\n",
    "        #      - k, l: key window spatial positions\n",
    "        #      - a, d, e: head dimensions (from h1, h2, h3)\n",
    "        #      - x, y, z: remaining TCL factors (flattened over window tokens)\n",
    "        attn = torch.einsum(\n",
    "            \"b m n i j a d e x y z, b m n k l a d e x y z -> b m n a d e i j k l\",\n",
    "            q, k\n",
    "        ) * self.scale\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # print(attn.shape)\n",
    "        num_tokens = ws1 * ws2\n",
    "\n",
    "        # 7. Compute and add relative positional bias.\n",
    "        bias = self.rel_bias_table[self.relative_position_index.view(-1)]\n",
    "        # (num_tokens, num_tokens, num_heads)\n",
    "        bias = bias.view(num_tokens, num_tokens, self.num_heads)\n",
    "        # (num_heads, num_tokens, num_tokens)\n",
    "        bias = bias.permute(2, 0, 1).contiguous()\n",
    "        # Reshape bias to separate head dimensions: (h1, h2, h3, num_tokens, num_tokens)\n",
    "        bias = bias.view(h1, h2, h3, num_tokens, num_tokens)\n",
    "        # (1, 1, 1, h1, h2, h3, num_tokens, num_tokens)\n",
    "        bias = bias.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "        # Flatten query spatial dims (i,j) and key spatial dims (k,l) in attn.\n",
    "        attn_flat = attn.view(B, nH, nW, h1, h2, h3, num_tokens, num_tokens)\n",
    "        attn_flat = attn_flat + bias\n",
    "\n",
    "        # print(attn_flat.shape)\n",
    "\n",
    "        # 8. Apply softmax over the key token dimension.\n",
    "        attn_softmax = torch.softmax(attn_flat, dim=-1)\n",
    "\n",
    "        # print(attn_softmax[0,0,0,0,0,0,0,:].sum())\n",
    "        # print(\"attnetion softmax\")\n",
    "        # print(attn_softmax.shape)\n",
    "        attn = attn_softmax.view(B, nH, nW, h1, h2, h3, ws1, ws2, ws1, ws2)\n",
    "\n",
    "\n",
    "        # print(\"v shape\" , v.shape)\n",
    "\n",
    "        # print(\"attn\" , attn.shape)\n",
    "        # print(\"attn shape\")\n",
    "\n",
    "        # print(attn[0, 0, 0, 0, 0, 0, 0, 0, :, :].sum())\n",
    "\n",
    "        # print(attn.shape)\n",
    "\n",
    "        # 9. Multiply attention scores with V to get the weighted sum.\n",
    "        final_output = torch.einsum(\n",
    "            \"b m n a d e i j k l, b m n i j a d e x y z -> b m n a d e k l x y z\",\n",
    "            attn, v\n",
    "        )\n",
    "        \n",
    "\n",
    "        # print(final_output.shape)\n",
    "\n",
    "        # print(final_output.shape)\n",
    "\n",
    "        # 10. Rearrange output to merge head dimensions and remaining TCL factors.\n",
    "        final_output_reshaped = rearrange(\n",
    "            final_output, \"b m n a d e k l x y z -> b m n k l (a x) (d y) (e z)\"\n",
    "        )\n",
    "\n",
    "        # print(final_output_reshaped.shape)\n",
    "\n",
    "        # print(final_output_reshaped.shape)\n",
    "\n",
    "        # 11. Reverse the window partition to reconstruct the full feature map.\n",
    "        out = self.window_partition.reverse(final_output_reshaped, H, W)\n",
    "\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f541b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size window (32, 7, 7, 2, 6, 8)\n",
      "shape of x torch.Size([2048, 7, 7, 2, 6, 8])\n",
      "shape of x torch.Size([2048, 7, 7, 2, 6, 8])\n",
      "shape of x torch.Size([2048, 7, 7, 2, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 56, 56, 2, 6, 8, device=\"cpu\")\n",
    "\n",
    "# Window parameters.\n",
    "window_size = 7\n",
    "# TCL configuration: for each window, input size is (7, 7, 4, 4, 3).\n",
    "# Here, we set rank_window to (4, 4, 3) and head_factors to (2, 2, 1) so that\n",
    "# the number of heads is 2*2*1 = 4.\n",
    "rank_window = (2,6,8)\n",
    "head_factors = (2, 3, 4)\n",
    "\n",
    "# Instantiate the W-MSA module.\n",
    "w_msa = WindowMSA(window_size=window_size, embed_dims=(2,6,8),\n",
    "                    rank_window=rank_window, head_factors=head_factors, device=\"cpu\").to(\"cpu\")\n",
    "\n",
    "x = w_msa(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
