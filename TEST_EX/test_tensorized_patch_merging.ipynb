{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d630b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TCL_CHANGED(nn.Module):\n",
    "    def __init__(self, input_size, rank, ignore_modes=(0,), bias=True, device='cuda'):\n",
    "        \"\"\"\n",
    "        input_size: tuple, shape of the input tensor (e.g., (7,7,4,4,3) for a window)\n",
    "        rank: tuple or int, target rank for the non-ignored modes (e.g., (4,4,3))\n",
    "        ignore_modes: tuple or int, indices of dimensions to leave unchanged (e.g., (0,1) to preserve spatial grid)\n",
    "        bias: bool, whether to add a bias parameter.\n",
    "        device: device string.\n",
    "        \"\"\"\n",
    "        super(TCL_CHANGED, self).__init__()\n",
    "        \n",
    "        alphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQERSUVWXYZ'\n",
    "        self.device = device\n",
    "        self.bias = bias\n",
    "        \n",
    "        if isinstance(input_size, int):\n",
    "            self.input_size = (input_size,)\n",
    "        else:\n",
    "            self.input_size = tuple(input_size)\n",
    "        \n",
    "        if isinstance(rank, int):\n",
    "            self.rank = (rank,)\n",
    "        else:\n",
    "            self.rank = tuple(rank)\n",
    "        \n",
    "        if isinstance(ignore_modes, int):\n",
    "            self.ignore_modes = (ignore_modes,)\n",
    "        else:\n",
    "            self.ignore_modes = tuple(ignore_modes)\n",
    "        \n",
    "        # Remove ignored modes from the input size\n",
    "        new_size = []\n",
    "        for i in range(len(self.input_size)):\n",
    "            if i in self.ignore_modes:\n",
    "                continue\n",
    "            else:\n",
    "                new_size.append(self.input_size[i])\n",
    "        \n",
    "        # Register bias if enabled\n",
    "        if self.bias:\n",
    "            # Bias shape is the same as the rank tensor\n",
    "            self.register_parameter('b', nn.Parameter(torch.empty(self.rank, device=self.device), requires_grad=True))\n",
    "        else:\n",
    "            self.register_parameter('b', None)\n",
    "            \n",
    "        # Register factor matrices (one per mode being contracted)\n",
    "        for i, r in enumerate(self.rank):\n",
    "            self.register_parameter(f'u{i}', nn.Parameter(torch.empty((r, new_size[i]), device=self.device), requires_grad=True))\n",
    "        \n",
    "        # Dynamically build the einsum formula for tensor contraction.\n",
    "        index = 0\n",
    "        formula = ''\n",
    "        core_str = ''\n",
    "        extend_str = ''\n",
    "        out_str = ''\n",
    "        # Build input part and track core (contracted) vs. extended (ignored) dimensions\n",
    "        for i in range(len(self.input_size)):\n",
    "            formula += alphabet[index]\n",
    "            if i not in self.ignore_modes:\n",
    "                core_str += alphabet[index]\n",
    "            else:\n",
    "                extend_str += alphabet[index]\n",
    "            index += 1\n",
    "            if i == len(self.input_size) - 1:\n",
    "                formula += ','\n",
    "        \n",
    "        # Build factor matrices part and output mapping\n",
    "        for l in range(len(self.rank)):\n",
    "            formula += alphabet[index]\n",
    "            formula += core_str[l]\n",
    "            out_str += alphabet[index]\n",
    "            index += 1\n",
    "            if l < len(self.rank) - 1:\n",
    "                formula += ','\n",
    "            elif l == len(self.rank) - 1:\n",
    "                formula += '->'\n",
    "        formula += extend_str + out_str\n",
    "        \n",
    "        self.out_formula = formula\n",
    "        # Uncomment the following line to inspect the generated einsum formula:\n",
    "        # print(\"Generated einsum formula:\", self.out_formula)\n",
    "\n",
    "        self.init_param()  # Initialize parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        If the input x has an extra batch dimension (i.e. its dimension equals len(input_size)+1),\n",
    "        insert an ellipsis in both the input and output parts of the einsum equation.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "        if x.dim() == len(self.input_size) + 1:\n",
    "            input_part, output_part = self.out_formula.split(\"->\")\n",
    "            new_formula = \"...\" + input_part + \"->...\" + output_part\n",
    "        else:\n",
    "            new_formula = self.out_formula\n",
    "        \n",
    "        operands = [x]\n",
    "        for i in range(len(self.rank)):\n",
    "            operands.append(getattr(self, f'u{i}'))\n",
    "        \n",
    "        out = torch.einsum(new_formula, *operands)\n",
    "        if self.bias:\n",
    "            out += self.b\n",
    "        return out\n",
    "\n",
    "    def init_param(self):\n",
    "        # Initialize factor matrices using Kaiming Uniform initialization\n",
    "        for i in range(len(self.rank)):\n",
    "            init.kaiming_uniform_(getattr(self, f'u{i}'), a=math.sqrt(5))\n",
    "        if self.bias:\n",
    "            bound = 1 / math.sqrt(self.input_size[0])\n",
    "            init.uniform_(self.b, -bound, bound)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc81ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TensorizedPatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Tensorized patch‑merging for Swin‑like models with flexible concatenation\n",
    "    along r1, r2 or C.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    input_size     : tuple  (B, H, W, r1, r2, C)\n",
    "    in_embed_shape : tuple  (r1, r2, C)          – shape of a **single** input patch\n",
    "    out_embed_shape: tuple  (r1', r2', C')       – target embedding shape per patch\n",
    "    channel_mode   : int    {0,1,2}\n",
    "                     0 → concatenate along r1  → (B, H/2, W/2, 4*r1,  r2,  C)\n",
    "                     1 → concatenate along r2  → (B, H/2, W/2,  r1, 4*r2,  C)\n",
    "                     2 → concatenate along C   → (B, H/2, W/2,  r1,  r2, 4*C)\n",
    "    bias           : bool\n",
    "    ignore_modes   : tuple – passed straight to TCL_CHANGED\n",
    "    device         : str\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=(16, 56, 56, 4, 4, 3),\n",
    "        in_embed_shape=(4, 4, 3),\n",
    "        out_embed_shape=(4, 4, 6),\n",
    "        channel_mode: int = 2,\n",
    "        bias: bool = True,\n",
    "        ignore_modes=(0, 1, 2),\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if channel_mode not in (0, 1, 2):\n",
    "            raise ValueError(\"`channel_mode` must be 0, 1 or 2.\")\n",
    "        self.channel_mode = channel_mode\n",
    "\n",
    "        # ----------  bookkeeping ----------\n",
    "        self.in_r1, self.in_r2, self.in_C = in_embed_shape\n",
    "        self.out_r1, self.out_r2, self.out_C = out_embed_shape\n",
    "        self.in_dim = self.in_r1 * self.in_r2 * self.in_C\n",
    "        self.out_dim = self.out_r1 * self.out_r2 * self.out_C\n",
    "\n",
    "        # usual Swin constraint (kept here in case you still rely on it)\n",
    "        if 4 * self.in_dim != 2 * self.out_dim:\n",
    "            raise ValueError(\n",
    "                f\"Dimension mismatch: expected out_dim = 2 * in_dim, got \"\n",
    "                f\"{self.out_dim} != {2 * self.in_dim}\"\n",
    "            )\n",
    "\n",
    "        self.ignore_modes = ignore_modes\n",
    "        self.bias = bias\n",
    "        self.device = device\n",
    "        self.input_size = input_size  # (B, H, W, r1, r2, C)\n",
    "\n",
    "        # ----------  sizes after merging ----------\n",
    "        B, H, W, r1, r2, C = self.input_size\n",
    "        if channel_mode == 0:        # expand r1\n",
    "            merged_shape = (4 * r1, r2, C)\n",
    "            cat_dim = 3\n",
    "        elif channel_mode == 1:      # expand r2\n",
    "            merged_shape = (r1, 4 * r2, C)\n",
    "            cat_dim = 4\n",
    "        else:                        # expand C\n",
    "            merged_shape = (r1, r2, 4 * C)\n",
    "            cat_dim = 5\n",
    "        self._cat_dim = cat_dim      # store for forward()\n",
    "\n",
    "        self.tcl_input_size = (B, H // 2, W // 2, *merged_shape)\n",
    "        self.norm = nn.LayerNorm(merged_shape)\n",
    "\n",
    "        # ----------  tensorized linear ----------\n",
    "        self.tcl = TCL_CHANGED(\n",
    "            input_size=self.tcl_input_size,\n",
    "            rank=out_embed_shape,\n",
    "            ignore_modes=self.ignore_modes,\n",
    "            bias=self.bias,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    #                                 forward                               #\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x : (B, H, W, r1, r2, C)\n",
    "        returns\n",
    "        ------\n",
    "        merged embeddings : (B, H/2, W/2, out_r1, out_r2, out_C)\n",
    "        \"\"\"\n",
    "        B, H, W, r1, r2, C = x.shape\n",
    "        if (r1, r2, C) != (self.in_r1, self.in_r2, self.in_C):\n",
    "            raise ValueError(\"Input patch embedding shape mismatch.\")\n",
    "\n",
    "        # 2×2 window split\n",
    "        tl = x[:, 0::2, 0::2]  # top‑left\n",
    "        bl = x[:, 1::2, 0::2]  # bottom‑left\n",
    "        tr = x[:, 0::2, 1::2]  # top‑right\n",
    "        br = x[:, 1::2, 1::2]  # bottom‑right\n",
    "\n",
    "        # concatenate along the requested mode\n",
    "        x_merged = torch.cat([tl, bl, tr, br], dim=self._cat_dim)\n",
    "\n",
    "\n",
    "        print(x_merged.shape)\n",
    "\n",
    "        # norm + tensorized linear projection\n",
    "        x_merged = self.norm(x_merged)\n",
    "        return self.tcl(x_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 7, 8, 6, 32])\n",
      "torch.Size([32, 7, 7, 2, 6, 64])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from Tensorized_Layers.TCL_CHANGED import TCL_CHANGED  # Replace with your actual TCL module\n",
    "# from your_module import TensorizedPatchMerging  # Replace with actual module/file name\n",
    "\n",
    "# Step 1: Create input tensor on CPU\n",
    "input_tensor = torch.randn(32, 14, 14, 2, 6, 32)  # Shape = (B, H, W, r1, r2, C)\n",
    "\n",
    "# Step 2: Instantiate the module with CPU device\n",
    "patch_merger = TensorizedPatchMerging(\n",
    "    input_size=(32, 14, 14, 2, 6, 32),\n",
    "    in_embed_shape=(2, 6, 32),\n",
    "    out_embed_shape=(2, 6, 64),\n",
    "    bias=True,\n",
    "    ignore_modes=(0, 1, 2),\n",
    "    device=\"cpu\",             \n",
    "    channel_mode=0            \n",
    ")\n",
    "\n",
    "# Step 3: Forward pass\n",
    "output_tensor = patch_merger(input_tensor)\n",
    "\n",
    "\n",
    "print(output_tensor.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
