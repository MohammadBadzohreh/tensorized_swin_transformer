{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e865125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "############################################################\n",
    "# 1) Drop Path Function & Module\n",
    "############################################################\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    \n",
    "    keep_prob = 1 - drop_prob\n",
    "    batch_size = x.shape[0]\n",
    "    # shape for random mask -> (B, 1, 1, 1, 1, 1) for your 6D input\n",
    "    random_tensor = keep_prob + torch.rand(\n",
    "        (batch_size, 1, 1, 1, 1, 1),\n",
    "        dtype=x.dtype, device=x.device\n",
    "    )\n",
    "    random_tensor.floor_()\n",
    "    x = x / keep_prob * random_tensor\n",
    "    return x\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl, embed_shape, dropout=0.0, drop_path_rate=0.0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl = tcl\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1) Window MSA + residual\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (2) TCL + residual\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (3) Shifted Window MSA + residual\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        # (4) TCL + residual\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl(x)\n",
    "        x = x_res + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "############################################################\n",
    "# 3) SwinTransformer passing drop_path_rate to the blocks\n",
    "############################################################\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 drop_path_rate=0.0,  # <---\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # patch embedding, etc...\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_shape=embed_shape,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "        # block 1\n",
    "        self.w_msa_1 = WindowMSA(...)\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(...)\n",
    "        self.tcl_1    = TCL_CHANGED(...)\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl=self.tcl_1,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout,\n",
    "                drop_path_rate=drop_path_rate  # pass in\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # block 2, block 3, block 4... do similarly\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(...)\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(..., drop_path_rate=drop_path_rate) \n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # etc...\n",
    "\n",
    "        self.classifier = TRL(...)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, 56, 56, 4, 4, 12, device=self.device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for blk in self.block1_list:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "        for blk in self.block2_list:\n",
    "            x = blk(x)\n",
    "        \n",
    "        # block3, block4, etc...\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
