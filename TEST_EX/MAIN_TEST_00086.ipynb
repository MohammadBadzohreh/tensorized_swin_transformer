{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "# convolution patch embedding\n",
    "# from Tensorized_components.patch_embedding  import Patch_Embedding        \n",
    "from Tensorized_components.tcl_patch_embedding  import  PatchEmbedding  as  Patch_Embedding      \n",
    "from Tensorized_components.w_msa_w_o_b_sign  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b_sign import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL  import TCL  as TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.cifar10_loaders_repeated_count import get_cifar10_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing 'Block 1' in your Swin Transformer.\n",
    "    This captures the sequence of:\n",
    "        (1) Window MSA + residual\n",
    "        (2) TCL + residual\n",
    "        (3) Shifted Window MSA + residual\n",
    "        (4) TCL + residual\n",
    "    but only for the first blockâ€™s hyperparameters and submodules.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,tcl2,tcl3,tcl4, embed_shape, dropout=0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        # Typically each sub-layer has its own LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We pass in pre-built modules (WindowMSA, ShiftedWindowMSA, TCL)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "    def forward(self, x):\n",
    "        # ----- First Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- Shifted Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4,  embed_shape=(4,4,6), dropout=0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "        # LN layers\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # Shifted Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock3(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1,  tcl2 , tcl3 , tcl4,   embed_shape=(4,4,12), dropout=0):\n",
    "        super(SwinBlock3, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock4(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, tcl1, tcl2 , tcl3 , tcl4 ,  embed_shape=(4,4,24), dropout=0):\n",
    "        super(SwinBlock4, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.tcl1 = tcl1\n",
    "        self.tcl2 = tcl2\n",
    "        self.tcl3 = tcl3\n",
    "        self.tcl4 = tcl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.tcl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.tcl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.tcl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "# tcl patch embedding \n",
    "\n",
    "\n",
    "# TODO : change batch size and device to cuda\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            input_size=(16,3,224,224),\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_shape,\n",
    "            bias=bias,\n",
    "            device=\"cpu\",\n",
    "            ignore_modes = (0,1,2)\n",
    "        )\n",
    "# convolution \n",
    "        # self.patch_embedding = Patch_Embedding(\n",
    "        #     img_size=img_size,\n",
    "        #     patch_size=patch_size,\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_shape=embed_shape,\n",
    "        #     bias=bias\n",
    "        # )\n",
    "\n",
    "        # -------------------------------- block 1 --------------------------\n",
    "\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_2 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_3 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_1_4 = TCL_CHANGED(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            rank=(4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                tcl1=self.tcl_1,\n",
    "                tcl2 = self.tcl_1_2,\n",
    "                tcl3 = self.tcl_1_3,\n",
    "                tcl4 = self.tcl_1_4,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- block 2 --------------------------\n",
    "\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(4,4,24),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.tcl_2_2 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_2_3 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.tcl_2_4 = TCL_CHANGED(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            rank=(4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # We repeat Block2 two times\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                tcl1=self.tcl_2,\n",
    "                tcl2 = self.tcl_2_2,\n",
    "                tcl3 = self.tcl_2_3,\n",
    "                tcl4 = self.tcl_2_4,\n",
    "                embed_shape=(4,4,24),  \n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # # -------------------------------- block 3 --------------------------\n",
    "\n",
    "        self.patch_merging_2 = TensorizedPatchMerging(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            in_embed_shape=(4,4,24),\n",
    "            out_embed_shape=(4,4,48),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.w_msa_3 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_3 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.tcl_3_2 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_3 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            rank=(4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_3_4 = TCL_CHANGED(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            rank=(4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Repeat Block3 6 times\n",
    "        self.block3_list = nn.ModuleList([\n",
    "            SwinBlock3(\n",
    "                w_msa=self.w_msa_3,\n",
    "                sw_msa=self.sw_msa_3,\n",
    "                tcl1=self.tcl_3,\n",
    "                tcl2=self.tcl_3_2,\n",
    "                tcl3=self.tcl_3_3,\n",
    "                tcl4=self.tcl_3_4,\n",
    "                embed_shape=(4,4,48),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(18)\n",
    "        ])\n",
    "\n",
    "        # # # -------------------------------- block 4 --------------------------\n",
    "\n",
    "        self.patch_merging_3 = TensorizedPatchMerging(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            in_embed_shape=(4,4,48),\n",
    "            out_embed_shape=(4,4,96),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_4 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_4 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.tcl_4_2 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_3 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            rank=(4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.tcl_4_4 = TCL_CHANGED(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            rank=(4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block4_list = nn.ModuleList([\n",
    "            SwinBlock4(\n",
    "                w_msa=self.w_msa_4,\n",
    "                sw_msa=self.sw_msa_4,\n",
    "                tcl1=self.tcl_4,\n",
    "                tcl2=self.tcl_4_2,\n",
    "                tcl3=self.tcl_4_3,\n",
    "                tcl4=self.tcl_4_4,\n",
    "                embed_shape=(4,4,96),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- classifier --------------------------\n",
    "\n",
    "    \n",
    "\n",
    "        self.classifier = TRL(input_size=(16,4,4,96),\n",
    "                            output=(200,),\n",
    "                            rank=(4,4,96,200),\n",
    "                            ignore_modes=(0,),\n",
    "                            bias=bias,\n",
    "                            device=self.device) \n",
    "        \n",
    "\n",
    "        # positoin embedding\n",
    "\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1,\n",
    "                        56,\n",
    "                        56,\n",
    "                        4,\n",
    "                        4,\n",
    "                        12,\n",
    "                        device = self.device\n",
    "                        ), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for i, blk in enumerate(self.block1_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block2_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_2(x)\n",
    "\n",
    "        for i, blk in enumerate(self.block3_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_3(x)\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block4_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID00086'\n",
    "batch_size = 16\n",
    "n_epoch = 400\n",
    "image_size = 224\n",
    "train_size = \"default\"\n",
    "\n",
    "model = SwinTransformer(img_size=224,patch_size=4,in_chans=3,embed_shape=(4,4,12),bias=True,device=device).to(device)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "            RandAugment(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            RandomErasing(p=0.25)\n",
    "        ])\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "train_loader, _ = get_cifar10_dataloaders('../datasets', transform_train, transform_test, batch_size, image_size, train_size, repeat_count=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 1700556 parameters\n",
      "Training for 400 epochs\n",
      "\n",
      "Train epoch 1: top1=0.29220399260520935%, top2=0.4878000020980835%, top3=0.6172000169754028%, top4=0.7176839709281921%, top5=0.7940199971199036%, loss=0.11803548775219917, time=5051.733330011368s\n",
      "Train epoch 2: top1=0.43363600969314575%, top2=0.6471999883651733%, top3=0.7660800218582153%, top4=0.8442919850349426%, top5=0.897812008857727%, loss=0.09753534256529808, time=5151.587424755096s\n",
      "Train epoch 3: top1=0.4778920114040375%, top2=0.6874600052833557%, top3=0.7990639805793762%, top4=0.8694400191307068%, top5=0.9164000153541565%, loss=0.09041774552774429, time=5215.132087469101s\n",
      "Train epoch 4: top1=0.509443998336792%, top2=0.7150239944458008%, top3=0.8204439878463745%, top4=0.8854960203170776%, top5=0.9280880093574524%, loss=0.08513424030017853, time=5218.1435260772705s\n",
      "Train epoch 5: top1=0.537227988243103%, top2=0.739300012588501%, top3=0.8375279903411865%, top4=0.8981680274009705%, top5=0.9371799826622009%, loss=0.08057556946468353, time=5217.5434901714325s\n",
      "Train epoch 6: top1=0.5611079931259155%, top2=0.7567239999771118%, top3=0.8503760099411011%, top4=0.9071800112724304%, top5=0.9432560205459595%, loss=0.0768683074234724, time=5216.623856782913s\n",
      "Train epoch 7: top1=0.5813480019569397%, top2=0.7735679745674133%, top3=0.8627679944038391%, top4=0.9162319898605347%, top5=0.9494760036468506%, loss=0.0735041782693863, time=5217.299574613571s\n",
      "Train epoch 8: top1=0.5985599756240845%, top2=0.7867640256881714%, top3=0.872596025466919%, top4=0.9232199788093567%, top5=0.9540759921073914%, loss=0.07038175923585892, time=5091.824106693268s\n",
      "Train epoch 9: top1=0.6131920218467712%, top2=0.7970679998397827%, top3=0.879580020904541%, top4=0.9279919862747192%, top5=0.9568799734115601%, loss=0.06807665686607361, time=5049.019670009613s\n",
      "Train epoch 10: top1=0.6269760131835938%, top2=0.807636022567749%, top3=0.8878080248832703%, top4=0.9337440133094788%, top5=0.9606879949569702%, loss=0.06571954838252067, time=5048.820092201233s\n",
      "Train epoch 11: top1=0.6377679705619812%, top2=0.8162279725074768%, top3=0.8937519788742065%, top4=0.9375200271606445%, top5=0.9631159901618958%, loss=0.06387124182569981, time=5053.376049280167s\n",
      "Train epoch 12: top1=0.6479640007019043%, top2=0.8223199844360352%, top3=0.8984519839286804%, top4=0.9409800171852112%, top5=0.9656919836997986%, loss=0.062025937614738944, time=5050.80602312088s\n",
      "Train epoch 13: top1=0.6587640047073364%, top2=0.8298959732055664%, top3=0.9039480090141296%, top4=0.9446479678153992%, top5=0.9681800007820129%, loss=0.06033406846064329, time=5060.52083158493s\n",
      "Train epoch 14: top1=0.6670240163803101%, top2=0.8368039727210999%, top3=0.9078999757766724%, top4=0.94651198387146%, top5=0.9688640236854553%, loss=0.058764170742988585, time=5223.231544971466s\n",
      "Train epoch 15: top1=0.6752399802207947%, top2=0.8421120047569275%, top3=0.9114159941673279%, top4=0.9492560029029846%, top5=0.9708560109138489%, loss=0.05735022832220793, time=5228.339729070663s\n",
      "Train epoch 16: top1=0.6834359765052795%, top2=0.847495973110199%, top3=0.9149879813194275%, top4=0.9517759680747986%, top5=0.972603976726532%, loss=0.05589625282895565, time=5225.829370737076s\n",
      "Train epoch 17: top1=0.6901199817657471%, top2=0.8518519997596741%, top3=0.9173719882965088%, top4=0.9531679749488831%, top5=0.9738159775733948%, loss=0.05477036447352171, time=5225.876783847809s\n",
      "Train epoch 18: top1=0.6973279714584351%, top2=0.8568879961967468%, top3=0.9211599826812744%, top4=0.9557880163192749%, top5=0.9751520156860352%, loss=0.053524160391151904, time=5220.675827264786s\n",
      "Train epoch 19: top1=0.7016839981079102%, top2=0.8609840273857117%, top3=0.9238280057907104%, top4=0.9574080109596252%, top5=0.9757000207901001%, loss=0.05273008568060398, time=5224.622284889221s\n",
      "Train epoch 20: top1=0.7077999711036682%, top2=0.8625879883766174%, top3=0.9256799817085266%, top4=0.9584680199623108%, top5=0.9766359925270081%, loss=0.05179046827146411, time=5224.953378915787s\n",
      "Train epoch 21: top1=0.7138040065765381%, top2=0.8668680191040039%, top3=0.9289680123329163%, top4=0.9610679745674133%, top5=0.9783999919891357%, loss=0.05060093725061417, time=5228.0532348155975s\n",
      "Train epoch 22: top1=0.7180479764938354%, top2=0.870203971862793%, top3=0.9301720261573792%, top4=0.9614920020103455%, top5=0.9783560037612915%, loss=0.049955913046658036, time=5223.615495920181s\n",
      "Train epoch 23: top1=0.7236359715461731%, top2=0.8736559748649597%, top3=0.9325640201568604%, top4=0.962444007396698%, top5=0.9790599942207336%, loss=0.049053378173321484, time=5228.860182285309s\n",
      "Train epoch 24: top1=0.7280480265617371%, top2=0.8767880201339722%, top3=0.9342560172080994%, top4=0.9640799760818481%, top5=0.9802839756011963%, loss=0.04820494106388092, time=5230.4977560043335s\n",
      "Train epoch 25: top1=0.7307159900665283%, top2=0.877843976020813%, top3=0.9356319904327393%, top4=0.9649680256843567%, top5=0.9809200167655945%, loss=0.04765274562332034, time=5230.099486589432s\n",
      "Train epoch 26: top1=0.7358120083808899%, top2=0.8804199695587158%, top3=0.9376959800720215%, top4=0.9661120176315308%, top5=0.9810320138931274%, loss=0.046890288975059986, time=5231.774399757385s\n",
      "Train epoch 27: top1=0.7392439842224121%, top2=0.883791983127594%, top3=0.9384199976921082%, top4=0.966372013092041%, top5=0.9813600182533264%, loss=0.046279058395832774, time=5231.118038892746s\n",
      "Train epoch 28: top1=0.7438200116157532%, top2=0.8858960270881653%, top3=0.9398040175437927%, top4=0.9669079780578613%, top5=0.9819080233573914%, loss=0.04556221479147673, time=5136.424882173538s\n",
      "Train epoch 29: top1=0.7457960247993469%, top2=0.8860599994659424%, top3=0.9403359889984131%, top4=0.9679319858551025%, top5=0.9825519919395447%, loss=0.04523920686423778, time=5068.986801624298s\n",
      "Train epoch 30: top1=0.7481039762496948%, top2=0.8892759680747986%, top3=0.9416919946670532%, top4=0.9684280157089233%, top5=0.9826719760894775%, loss=0.0447709657407701, time=5142.061631679535s\n",
      "Train epoch 31: top1=0.7519320249557495%, top2=0.8917999863624573%, top3=0.9434679746627808%, top4=0.9695119857788086%, top5=0.9834280014038086%, loss=0.044107298086196185, time=5255.111871004105s\n",
      "Train epoch 32: top1=0.7540760040283203%, top2=0.8931360244750977%, top3=0.9448000192642212%, top4=0.9701600074768066%, top5=0.983951985836029%, loss=0.043599719875380395, time=5202.408083200455s\n",
      "Train epoch 33: top1=0.7547119855880737%, top2=0.8936479687690735%, top3=0.9456120133399963%, top4=0.9711040258407593%, top5=0.9842479825019836%, loss=0.043413026534110305, time=5058.453864574432s\n",
      "Train epoch 34: top1=0.7589920163154602%, top2=0.8959119915962219%, top3=0.9464200139045715%, top4=0.9714840054512024%, top5=0.9846360087394714%, loss=0.04275162235426903, time=5074.935069561005s\n",
      "Train epoch 35: top1=0.761132001876831%, top2=0.897059977054596%, top3=0.9469000101089478%, top4=0.9717360138893127%, top5=0.9844880104064941%, loss=0.04236995497271419, time=5055.757874250412s\n",
      "Train epoch 36: top1=0.7627279758453369%, top2=0.896996021270752%, top3=0.9475240111351013%, top4=0.9722920060157776%, top5=0.9849839806556702%, loss=0.04225697799000144, time=5062.876404762268s\n",
      "Train epoch 37: top1=0.7639560103416443%, top2=0.8997520208358765%, top3=0.9489759802818298%, top4=0.9724720120429993%, top5=0.9853559732437134%, loss=0.04186008327049017, time=5058.961691617966s\n",
      "Train epoch 38: top1=0.7670400142669678%, top2=0.9003000259399414%, top3=0.9494159817695618%, top4=0.9729800224304199%, top5=0.9853439927101135%, loss=0.04138994691862166, time=5061.280001878738s\n",
      "Train epoch 39: top1=0.7691559791564941%, top2=0.9017800092697144%, top3=0.949836015701294%, top4=0.9732080101966858%, top5=0.985975980758667%, loss=0.041021449631869794, time=5061.883502006531s\n",
      "Train epoch 40: top1=0.7695080041885376%, top2=0.9019920229911804%, top3=0.9500280022621155%, top4=0.9734039902687073%, top5=0.9856039881706238%, loss=0.040947098884165284, time=5060.8187420368195s\n",
      "Train epoch 41: top1=0.7720479965209961%, top2=0.9027479887008667%, top3=0.9502400159835815%, top4=0.9734320044517517%, top5=0.9857239723205566%, loss=0.040545036386340855, time=5065.70282626152s\n",
      "Train epoch 42: top1=0.772167980670929%, top2=0.90419602394104%, top3=0.9512919783592224%, top4=0.9743319749832153%, top5=0.9863600134849548%, loss=0.04035540903410315, time=5063.809597015381s\n",
      "Train epoch 43: top1=0.775048017501831%, top2=0.9052000045776367%, top3=0.952131986618042%, top4=0.9745720028877258%, top5=0.9866880178451538%, loss=0.04005173579192162, time=5067.134278059006s\n",
      "Train epoch 44: top1=0.7753599882125854%, top2=0.90556401014328%, top3=0.9516000151634216%, top4=0.9746879935264587%, top5=0.9865880012512207%, loss=0.039852570983052255, time=5062.531368732452s\n",
      "Train epoch 45: top1=0.7784960269927979%, top2=0.9069560170173645%, top3=0.9527400135993958%, top4=0.9751880168914795%, top5=0.9865720272064209%, loss=0.039385430602610114, time=5065.863906145096s\n",
      "Train epoch 46: top1=0.7786440253257751%, top2=0.90665602684021%, top3=0.9523999691009521%, top4=0.9748560190200806%, top5=0.9864199757575989%, loss=0.03939179049050808, time=5065.504907369614s\n",
      "Train epoch 47: top1=0.7804319858551025%, top2=0.9076560139656067%, top3=0.9532840251922607%, top4=0.9754840135574341%, top5=0.9869840145111084%, loss=0.03912340046870708, time=5061.757202863693s\n",
      "Train epoch 48: top1=0.7808719873428345%, top2=0.9085479974746704%, top3=0.9538879990577698%, top4=0.9755640029907227%, top5=0.9870520234107971%, loss=0.03907037611334026, time=5065.743050098419s\n",
      "Train epoch 49: top1=0.7822999954223633%, top2=0.9095360040664673%, top3=0.9544640183448792%, top4=0.9762279987335205%, top5=0.9871959686279297%, loss=0.038597433203935626, time=5068.652099132538s\n",
      "Train epoch 50: top1=0.7836480140686035%, top2=0.909887969493866%, top3=0.9544640183448792%, top4=0.9759320020675659%, top5=0.9871360063552856%, loss=0.03855706387640536, time=5069.618693828583s\n",
      "Train epoch 51: top1=0.7842159867286682%, top2=0.910975992679596%, top3=0.9549239873886108%, top4=0.9762920141220093%, top5=0.987555980682373%, loss=0.03831163830599189, time=5066.793330669403s\n",
      "Train epoch 52: top1=0.7844480276107788%, top2=0.9107999801635742%, top3=0.9555240273475647%, top4=0.9766799807548523%, top5=0.9877439737319946%, loss=0.03826292497988045, time=5064.511323451996s\n",
      "Train epoch 53: top1=0.7849760055541992%, top2=0.9115319848060608%, top3=0.956063985824585%, top4=0.9766600131988525%, top5=0.9878439903259277%, loss=0.03814027898320556, time=5060.013708591461s\n",
      "Train epoch 54: top1=0.7868800163269043%, top2=0.9123520255088806%, top3=0.9563199877738953%, top4=0.9771919846534729%, top5=0.9880880117416382%, loss=0.03777960127711296, time=5056.54039812088s\n",
      "Train epoch 55: top1=0.7875840067863464%, top2=0.9129959940910339%, top3=0.956167995929718%, top4=0.9770960211753845%, top5=0.9878680109977722%, loss=0.03770013576180488, time=5060.950129270554s\n",
      "Train epoch 56: top1=0.788096010684967%, top2=0.9128479957580566%, top3=0.9562320113182068%, top4=0.9775959849357605%, top5=0.9880840182304382%, loss=0.03769566793707013, time=5062.084119796753s\n",
      "Train epoch 57: top1=0.7883599996566772%, top2=0.9135519862174988%, top3=0.95660799741745%, top4=0.9772800207138062%, top5=0.9877079725265503%, loss=0.03749856768518686, time=5066.146413326263s\n",
      "Train epoch 58: top1=0.7918679714202881%, top2=0.914143979549408%, top3=0.9569000005722046%, top4=0.9774879813194275%, top5=0.9878360033035278%, loss=0.03717820234489441, time=5067.332996845245s\n",
      "Train epoch 59: top1=0.7904520034790039%, top2=0.9150760173797607%, top3=0.9577639698982239%, top4=0.9775599837303162%, top5=0.9880719780921936%, loss=0.03717753385038674, time=5061.828875541687s\n",
      "Train epoch 60: top1=0.7923280000686646%, top2=0.9149680137634277%, top3=0.9579079747200012%, top4=0.9782320261001587%, top5=0.9885200262069702%, loss=0.03693192587539554, time=5058.903886795044s\n",
      "Train epoch 61: top1=0.792572021484375%, top2=0.9154639840126038%, top3=0.9581720232963562%, top4=0.9784079790115356%, top5=0.9887279868125916%, loss=0.03687126922395825, time=5065.132980108261s\n",
      "Train epoch 62: top1=0.7939959764480591%, top2=0.9158639907836914%, top3=0.9578840136528015%, top4=0.9780600070953369%, top5=0.9884079694747925%, loss=0.03664269860929251, time=5067.736723184586s\n",
      "Train epoch 63: top1=0.7942720055580139%, top2=0.916271984577179%, top3=0.9582920074462891%, top4=0.9787439703941345%, top5=0.9888280034065247%, loss=0.03645642556295544, time=5061.778279304504s\n",
      "Train epoch 64: top1=0.794867992401123%, top2=0.9165440201759338%, top3=0.9588680267333984%, top4=0.9784520268440247%, top5=0.988647997379303%, loss=0.03648058346131444, time=5081.50156545639s\n",
      "Train epoch 65: top1=0.7945119738578796%, top2=0.9173319935798645%, top3=0.9591919779777527%, top4=0.9789239764213562%, top5=0.9885720014572144%, loss=0.036403458264768124, time=5214.903390645981s\n",
      "Train epoch 66: top1=0.7962639927864075%, top2=0.9181479811668396%, top3=0.9595999717712402%, top4=0.9787520170211792%, top5=0.9890279769897461%, loss=0.03614380147065222, time=5257.636966466904s\n",
      "Train epoch 67: top1=0.7982839941978455%, top2=0.9183840155601501%, top3=0.9596880078315735%, top4=0.9795240163803101%, top5=0.9894480109214783%, loss=0.03588658284807205, time=5250.087481975555s\n",
      "Train epoch 68: top1=0.798151969909668%, top2=0.9192720055580139%, top3=0.9607239961624146%, top4=0.9796119928359985%, top5=0.9892600178718567%, loss=0.03590724824213982, time=5250.536558866501s\n",
      "Train epoch 69: top1=0.7976599931716919%, top2=0.9181519746780396%, top3=0.9598720073699951%, top4=0.9787480235099792%, top5=0.9887959957122803%, loss=0.03590374228462577, time=5250.9537608623505s\n",
      "Train epoch 70: top1=0.7982040047645569%, top2=0.9190279841423035%, top3=0.9599599838256836%, top4=0.9790999889373779%, top5=0.9890040159225464%, loss=0.035858326516211035, time=5251.942881345749s\n",
      "Train epoch 71: top1=0.8000360131263733%, top2=0.9194440245628357%, top3=0.9598519802093506%, top4=0.9790040254592896%, top5=0.9889240264892578%, loss=0.03562524624432623, time=5248.6708018779755s\n",
      "Train epoch 72: top1=0.800603985786438%, top2=0.9196040034294128%, top3=0.9605519771575928%, top4=0.9793919920921326%, top5=0.9890879988670349%, loss=0.03551860374386608, time=5256.2810344696045s\n",
      "Train epoch 73: top1=0.8006839752197266%, top2=0.9199119806289673%, top3=0.9607399702072144%, top4=0.9794719815254211%, top5=0.9891840219497681%, loss=0.03546064111869037, time=5253.642030000687s\n",
      "Train epoch 74: top1=0.801584005355835%, top2=0.9206839799880981%, top3=0.9607399702072144%, top4=0.9796239733695984%, top5=0.9891759753227234%, loss=0.035272497648254034, time=5255.934642791748s\n",
      "Train epoch 75: top1=0.8015360236167908%, top2=0.9202960133552551%, top3=0.9613999724388123%, top4=0.9800440073013306%, top5=0.9895399808883667%, loss=0.035194139101415875, time=5255.023965835571s\n",
      "Train epoch 76: top1=0.8013280034065247%, top2=0.9204279780387878%, top3=0.9605559706687927%, top4=0.9799799919128418%, top5=0.989575982093811%, loss=0.035192647189378735, time=5255.334313631058s\n",
      "Train epoch 77: top1=0.8043040037155151%, top2=0.9215279817581177%, top3=0.9611200094223022%, top4=0.9797840118408203%, top5=0.989300012588501%, loss=0.034977146987468004, time=5255.970352888107s\n",
      "Train epoch 78: top1=0.8035200238227844%, top2=0.9211639761924744%, top3=0.9616959691047668%, top4=0.9800599813461304%, top5=0.9893280267715454%, loss=0.03499723005090654, time=5250.108114004135s\n",
      "Train epoch 79: top1=0.8049719929695129%, top2=0.9221559762954712%, top3=0.9616919755935669%, top4=0.9802640080451965%, top5=0.9897279739379883%, loss=0.034854016566857694, time=5248.987690448761s\n",
      "Train epoch 80: top1=0.8031079769134521%, top2=0.9215480089187622%, top3=0.961899995803833%, top4=0.9805480241775513%, top5=0.9897159934043884%, loss=0.03488968932333589, time=5252.145474433899s\n",
      "Train epoch 81: top1=0.8045639991760254%, top2=0.922648012638092%, top3=0.9620599746704102%, top4=0.980675995349884%, top5=0.9897239804267883%, loss=0.034633805022493004, time=5252.914666652679s\n",
      "Train epoch 82: top1=0.8053920269012451%, top2=0.9224600195884705%, top3=0.962440013885498%, top4=0.9806919693946838%, top5=0.9898480176925659%, loss=0.03456175928629935, time=5247.252732038498s\n",
      "Train epoch 83: top1=0.805899977684021%, top2=0.9227839708328247%, top3=0.9627119898796082%, top4=0.9806839823722839%, top5=0.9898679852485657%, loss=0.03449171970910579, time=5179.587685823441s\n",
      "Train epoch 84: top1=0.8064720034599304%, top2=0.9231359958648682%, top3=0.9626920223236084%, top4=0.9809119701385498%, top5=0.9900720119476318%, loss=0.03440255027087033, time=5132.4061505794525s\n",
      "Train epoch 85: top1=0.8067359924316406%, top2=0.9240279793739319%, top3=0.9626479744911194%, top4=0.9808040261268616%, top5=0.9898080229759216%, loss=0.03426504365861416, time=5129.404379606247s\n",
      "Train epoch 86: top1=0.8077520132064819%, top2=0.9234679937362671%, top3=0.9630280137062073%, top4=0.981436014175415%, top5=0.9904040098190308%, loss=0.03413517730975151, time=5128.605548381805s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,n_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     report_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# report_test = test_epoch(test_loader, epoch)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     report \u001b[38;5;241m=\u001b[39m report_train \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#+ report_test + '\\n\\n'\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m topk_accuracy(outputs, targets, topk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m accuracies:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "# Define train and test functions (use examples)\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # print(f'batch{i} done!')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = f'Train epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_train)\n",
    "\n",
    "    return report_train\n",
    "\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
