{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optim\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\__init__.py:2222\u001b[0m\n\u001b[0;32m   2218\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, classes)\n\u001b[0;32m   2220\u001b[0m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\quantization\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuse_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuse_modules\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\quantization\\fake_quantize.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mhere.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_quantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     _is_fake_quant_script_module,\n\u001b[0;32m     12\u001b[0m     _is_per_channel,\n\u001b[0;32m     13\u001b[0m     _is_per_tensor,\n\u001b[0;32m     14\u001b[0m     _is_symmetric_quant,\n\u001b[0;32m     15\u001b[0m     default_fake_quant,\n\u001b[0;32m     16\u001b[0m     default_fixed_qparams_range_0to1_fake_quant,\n\u001b[0;32m     17\u001b[0m     default_fixed_qparams_range_neg1to1_fake_quant,\n\u001b[0;32m     18\u001b[0m     default_fused_act_fake_quant,\n\u001b[0;32m     19\u001b[0m     default_fused_per_channel_wt_fake_quant,\n\u001b[0;32m     20\u001b[0m     default_fused_wt_fake_quant,\n\u001b[0;32m     21\u001b[0m     default_histogram_fake_quant,\n\u001b[0;32m     22\u001b[0m     default_per_channel_weight_fake_quant,\n\u001b[0;32m     23\u001b[0m     default_weight_fake_quant,\n\u001b[0;32m     24\u001b[0m     disable_fake_quant,\n\u001b[0;32m     25\u001b[0m     disable_observer,\n\u001b[0;32m     26\u001b[0m     enable_fake_quant,\n\u001b[0;32m     27\u001b[0m     enable_observer,\n\u001b[0;32m     28\u001b[0m     FakeQuantize,\n\u001b[0;32m     29\u001b[0m     FakeQuantizeBase,\n\u001b[0;32m     30\u001b[0m     FixedQParamsFakeQuantize,\n\u001b[0;32m     31\u001b[0m     FusedMovingAvgObsFakeQuantize,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\ao\\quantization\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_numeric_debugger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     compare_results,\n\u001b[0;32m     14\u001b[0m     CUSTOM_KEY,\n\u001b[0;32m     15\u001b[0m     extract_results_from_loggers,\n\u001b[0;32m     16\u001b[0m     generate_numeric_debug_handle,\n\u001b[0;32m     17\u001b[0m     NUMERIC_DEBUG_HANDLE_KEY,\n\u001b[0;32m     18\u001b[0m     prepare_for_propagation_comparison,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     _allow_exported_model_train_eval \u001b[38;5;28;01mas\u001b[39;00m allow_exported_model_train_eval,\n\u001b[0;32m     22\u001b[0m     _move_exported_model_to_eval \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_eval,\n\u001b[0;32m     23\u001b[0m     _move_exported_model_to_train \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_train,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\_numeric_debugger.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_sqnr\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_control_flow_submodules\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule, Node\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\graph_utils.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource_matcher_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     check_subgraphs_connected,\n\u001b[0;32m     10\u001b[0m     get_source_partitions,\n\u001b[0;32m     11\u001b[0m     SourcePartition,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_sequential_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_control_flow_submodules\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_equivalent_types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_equivalent_types_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m _EQUIVALENT_TYPES: List[Set] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv1d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv1d},\n\u001b[0;32m     24\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv2d},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mimul, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul_\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     31\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\fx\\passes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     graph_drawer,\n\u001b[0;32m      3\u001b[0m     graph_manipulation,\n\u001b[0;32m      4\u001b[0m     net_min_base,\n\u001b[0;32m      5\u001b[0m     operator_support,\n\u001b[0;32m      6\u001b[0m     param_fetch,\n\u001b[0;32m      7\u001b[0m     reinplace,\n\u001b[0;32m      8\u001b[0m     runtime_assert,\n\u001b[0;32m      9\u001b[0m     shape_prop,\n\u001b[0;32m     10\u001b[0m     split_module,\n\u001b[0;32m     11\u001b[0m     split_utils,\n\u001b[0;32m     12\u001b[0m     splitter_base,\n\u001b[0;32m     13\u001b[0m     tools_common,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\data_science\\Lib\\site-packages\\torch\\fx\\passes\\graph_drawer.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_prop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorMetadata\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydot\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     HAS_PYDOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1532\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1506\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1605\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import os\n",
    "# convolution patch embedding\n",
    "# from Tensorized_components.patch_embedding  import Patch_Embedding        \n",
    "from Tensorized_components.tcl_patch_embedding  import  PatchEmbedding  as  Patch_Embedding      \n",
    "from Tensorized_components.w_msa_w_o_b_sign  import WindowMSA     \n",
    "from Tensorized_components.sh_wmsa_w_o_b_sign import ShiftedWindowMSA     \n",
    "from Tensorized_components.patch_merging  import TensorizedPatchMerging  \n",
    "from Tensorized_Layers.TCL  import TCL  as TCL_CHANGED   \n",
    "from Tensorized_Layers.TRL import TRL   \n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.tinyimagenet_loaders_repeat_count import get_tinyimagenet_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n",
    "from torchvision.transforms import RandAugment, RandomErasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing 'Block 1' in your Swin Transformer.\n",
    "    This captures the sequence of:\n",
    "        (1) Window MSA + residual\n",
    "        (2) TCL + residual\n",
    "        (3) Shifted Window MSA + residual\n",
    "        (4) TCL + residual\n",
    "    but only for the first block’s hyperparameters and submodules.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_msa, sw_msa, trl1,trl2,trl3,trl4, embed_shape, dropout=0):\n",
    "        super(SwinBlock1, self).__init__()\n",
    "        # Typically each sub-layer has its own LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We pass in pre-built modules (WindowMSA, ShiftedWindowMSA, TCL)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.trl1 = trl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.trl2 = trl2\n",
    "        self.trl3 = trl3\n",
    "        self.trl4 = trl4\n",
    "    def forward(self, x):\n",
    "        # ----- First Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.trl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- Shifted Window MSA + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # ----- TCL + Residual -----\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.trl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock2(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, trl1, trl2 , trl3 , trl4,  embed_shape=(4,4,6), dropout=0):\n",
    "        super(SwinBlock2, self).__init__()\n",
    "        # LN layers\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.trl1 = trl1\n",
    "        self.trl2 = trl2\n",
    "        self.trl3 = trl3\n",
    "        self.trl4 = trl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.trl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        # Shifted Window MSA\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # TCL\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.trl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock3(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, trl1,  trl2 , trl3 , trl4,   embed_shape=(4,4,12), dropout=0):\n",
    "        super(SwinBlock3, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.trl1 = trl1\n",
    "        self.gelu = nn.GELU()\n",
    "        self.trl2 = trl2\n",
    "        self.trl3 = trl3\n",
    "        self.trl4 = trl4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.trl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.trl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl4(x)\n",
    "        x = x + x_res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock4(nn.Module):\n",
    "    def __init__(self, w_msa, sw_msa, trl1, trl2 , trl3 , trl4 ,  embed_shape=(4,4,24), dropout=0):\n",
    "        super(SwinBlock4, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_shape)\n",
    "        self.norm2 = nn.LayerNorm(embed_shape)\n",
    "        self.norm3 = nn.LayerNorm(embed_shape)\n",
    "        self.norm4 = nn.LayerNorm(embed_shape)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_msa = w_msa\n",
    "        self.sw_msa = sw_msa\n",
    "        self.trl1 = trl1\n",
    "        self.trl2 = trl2\n",
    "        self.trl3 = trl3\n",
    "        self.trl4 = trl4\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(self.w_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.trl1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl2(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.dropout(self.sw_msa(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = x\n",
    "        x = self.norm4(x)\n",
    "        x = self.trl3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.trl4(x)\n",
    "        x = x + x_res\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_shape=(4,4,12),\n",
    "                 bias=True,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\"):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "# tcl patch embedding \n",
    "\n",
    "\n",
    "# TODO : change batch size and device to cuda\n",
    "        self.patch_embedding = Patch_Embedding(\n",
    "            input_size=(16,3,224,224),\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=embed_shape,\n",
    "            bias=bias,\n",
    "            device=\"cpu\",\n",
    "            ignore_modes = (0,1,2)\n",
    "        )\n",
    "# convolution \n",
    "        # self.patch_embedding = Patch_Embedding(\n",
    "        #     img_size=img_size,\n",
    "        #     patch_size=patch_size,\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_shape=embed_shape,\n",
    "        #     bias=bias\n",
    "        # )\n",
    "\n",
    "        # -------------------------------- block 1 --------------------------\n",
    "\n",
    "        self.w_msa_1 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_1 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=embed_shape,\n",
    "            rank_window=embed_shape,\n",
    "            head_factors=(1,2,3),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "    # def __init__(self, input_size, output, rank, ignore_modes = (0,), bias = True, device = 'cuda'):\n",
    "\n",
    "        self.trl_1 = TRL(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            output=(4,4,48),\n",
    "            rank=(4,4,12,4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_1_2 = TRL(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            output=(4,4,12),\n",
    "            rank=(4,4,48,4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_1_3 = TRL(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            output=(4,4,48),\n",
    "            rank=(4,4,12,4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_1_4 = TRL(\n",
    "            input_size=(16, 56, 56, 4,4,48),\n",
    "            output=(4,4,12),\n",
    "            rank=(4,4,48,4,4,12),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1_list = nn.ModuleList([\n",
    "            SwinBlock1(\n",
    "                w_msa=self.w_msa_1,\n",
    "                sw_msa=self.sw_msa_1,\n",
    "                trl1=self.trl_1,\n",
    "                trl2 = self.trl_1_2,\n",
    "                trl3 = self.trl_1_3,\n",
    "                trl4 = self.trl_1_4,\n",
    "                embed_shape=embed_shape,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- block 2 --------------------------\n",
    "\n",
    "\n",
    "        self.patch_merging_1 = TensorizedPatchMerging(\n",
    "            input_size=(16, 56, 56, 4,4,12),\n",
    "            in_embed_shape=embed_shape,\n",
    "            out_embed_shape=(4,4,24),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_2 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_2 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,24),\n",
    "            rank_window=(4,4,24),\n",
    "            head_factors=(1,2,6),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.trl_2 = TRL(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            output=(4,4,96),\n",
    "            rank=(4,4,24,4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.trl_2_2 = TRL(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            output=(4,4,24),\n",
    "            rank=(4,4,96,4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_2_3 = TRL(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            output=(4,4,96),\n",
    "            rank=(4,4,24,4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        self.trl_2_4 = TRL(\n",
    "            input_size=(16, 28, 28, 4,4,96),\n",
    "            output=(4,4,24),\n",
    "            rank=(4,4,96,4,4,24),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        # We repeat Block2 two times\n",
    "        self.block2_list = nn.ModuleList([\n",
    "            SwinBlock2(\n",
    "                w_msa=self.w_msa_2,\n",
    "                sw_msa=self.sw_msa_2,\n",
    "                trl1=self.trl_2,\n",
    "                trl2 = self.trl_2_2,\n",
    "                trl3 = self.trl_2_3,\n",
    "                trl4 = self.trl_2_4,\n",
    "                embed_shape=(4,4,24),  \n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # # -------------------------------- block 3 --------------------------\n",
    "\n",
    "        self.patch_merging_2 = TensorizedPatchMerging(\n",
    "            input_size=(16, 28, 28, 4,4,24),\n",
    "            in_embed_shape=(4,4,24),\n",
    "            out_embed_shape=(4,4,48),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.w_msa_3 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_3 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,48),\n",
    "            rank_window=(4,4,48),\n",
    "            head_factors=(2,1,12),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.trl_3 = TRL(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            output=(4,4,192),\n",
    "            rank=(4,4,48,4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.trl_3_2 = TRL(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            output=(4,4,48),\n",
    "            rank=(4,4,192 ,4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_3_3 = TRL(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            output=(4,4,192),\n",
    "            rank=(4,4,48,4,4,192),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.trl_3_4 = TRL(\n",
    "            input_size=(16, 14, 14, 4,4,192),\n",
    "            output=(4,4,48),\n",
    "            rank=(4,4,192 ,4,4,48),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Repeat Block3 6 times\n",
    "        self.block3_list = nn.ModuleList([\n",
    "            SwinBlock3(\n",
    "                w_msa=self.w_msa_3,\n",
    "                sw_msa=self.sw_msa_3,\n",
    "                trl1=self.trl_3,\n",
    "                trl2=self.trl_3_2,\n",
    "                trl3=self.trl_3_3,\n",
    "                trl4=self.trl_3_4,\n",
    "                embed_shape=(4,4,48),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(18)\n",
    "        ])\n",
    "\n",
    "        # # # -------------------------------- block 4 --------------------------\n",
    "\n",
    "        self.patch_merging_3 = TensorizedPatchMerging(\n",
    "            input_size=(16, 14, 14, 4,4,48),\n",
    "            in_embed_shape=(4,4,48),\n",
    "            out_embed_shape=(4,4,96),\n",
    "            bias=bias,\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.w_msa_4 = WindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.sw_msa_4 = ShiftedWindowMSA(\n",
    "            window_size=7,\n",
    "            embed_dims=(4,4,96),\n",
    "            rank_window=(4,4,96),\n",
    "            head_factors=(2,1,24),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.trl_4 = TRL(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            output=(4,4,384),\n",
    "            rank=(4,4,96,4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.trl_4_2 = TRL(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            output = (4,4,96),\n",
    "            rank=(4,4,384,4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.trl_4_3 = TRL(\n",
    "            input_size=(16, 7, 7, 4,4,96),\n",
    "            output=(4,4,384),\n",
    "            rank=(4,4,96,4,4,384),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "\n",
    "        self.trl_4_4 = TRL(\n",
    "            input_size=(16, 7, 7, 4,4,384),\n",
    "            output = (4,4,96),\n",
    "            rank=(4,4,384,4,4,96),\n",
    "            ignore_modes=(0, 1, 2),\n",
    "            bias=bias,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.block4_list = nn.ModuleList([\n",
    "            SwinBlock4(\n",
    "                w_msa=self.w_msa_4,\n",
    "                sw_msa=self.sw_msa_4,\n",
    "                trl1=self.trl_4,\n",
    "                trl2=self.trl_4_2,\n",
    "                trl3=self.trl_4_3,\n",
    "                trl4=self.trl_4_4,\n",
    "                embed_shape=(4,4,96),\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # -------------------------------- classifier --------------------------\n",
    "\n",
    "    \n",
    "\n",
    "        self.classifier = TRL(input_size=(16,4,4,96),\n",
    "                            output=(200,),\n",
    "                            rank=(4,4,96,200),\n",
    "                            ignore_modes=(0,),\n",
    "                            bias=bias,\n",
    "                            device=self.device) \n",
    "        \n",
    "\n",
    "        # positoin embedding\n",
    "\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1,\n",
    "                        56,\n",
    "                        56,\n",
    "                        4,\n",
    "                        4,\n",
    "                        12,\n",
    "                        device = self.device\n",
    "                        ), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        for i, blk in enumerate(self.block1_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_1(x)\n",
    "\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block2_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_2(x)\n",
    "\n",
    "        for i, blk in enumerate(self.block3_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = self.patch_merging_3(x)\n",
    "\n",
    "\n",
    "        for i, blk in enumerate(self.block4_list, 1):\n",
    "            x = blk(x)\n",
    "\n",
    "\n",
    "        x = x.mean(dim=(1, 2))\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to : cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Setup the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(f'Device is set to : {device}')\n",
    "\n",
    "# Configs\n",
    "\n",
    "TEST_ID = 'Test_ID00034'\n",
    "batch_size = 16\n",
    "n_epoch = 400\n",
    "image_size = 224\n",
    "\n",
    "model = SwinTransformer(img_size=224,patch_size=4,in_chans=3,embed_shape=(4,4,12),bias=True,device=device).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tiny_transform_train = transforms.Compose([\n",
    "            RandAugment(), \n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            RandomErasing(p=0.25)\n",
    "        ])\n",
    "tiny_transform_val = transform_test = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "train_loader, _, _ = get_tinyimagenet_dataloaders('../datasets', tiny_transform_train, tiny_transform_val, transform_test, batch_size, image_size, repeat_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model has 52472588 parameters\n",
      "Training for 400 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters(model)\n",
    "print(f'This Model has {num_parameters} parameters')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "# Define train and test functions (use examples)\n",
    "def train_epoch(loader, epoch):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "        # print(f'batch{i} done!')\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_train = f'Train epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_train)\n",
    "\n",
    "    return report_train\n",
    "\n",
    "def test_epoch(loader, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    correct = {1:0.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0} # set the initial correct count for top1-to-top5 accuracy\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        accuracies = topk_accuracy(outputs, targets, topk=(1, 2, 3, 4, 5))\n",
    "        for k in accuracies:\n",
    "            correct[k] += accuracies[k]['correct']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    top1_acc, top2_acc, top3_acc, top4_acc, top5_acc = [(correct[k]/len(loader.dataset)) for k in correct]\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    report_test = f'Test epoch {epoch}: top1={top1_acc}%, top2={top2_acc}%, top3={top3_acc}%, top4={top4_acc}%, top5={top5_acc}%, loss={avg_loss}, time={elapsed_time}s'\n",
    "    print(report_test)\n",
    "\n",
    "    return report_test\n",
    "\n",
    "# Set up the directories to save the results\n",
    "result_dir = os.path.join('../results', TEST_ID)\n",
    "result_subdir = os.path.join(result_dir, 'accuracy_stats')\n",
    "model_subdir = os.path.join(result_dir, 'model_stats')\n",
    "\n",
    "os.makedirs(result_subdir, exist_ok=True)\n",
    "os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_dir, 'model_stats', 'model_info.txt'), 'a') as f:\n",
    "    f.write(f'total number of parameters:\\n{num_parameters}')\n",
    "\n",
    "# Train from Scratch - Just Train\n",
    "print(f'Training for {len(range(n_epoch))} epochs\\n')\n",
    "for epoch in range(0+1,n_epoch+1):\n",
    "    report_train = train_epoch(train_loader, epoch)\n",
    "    # report_test = test_epoch(test_loader, epoch)\n",
    "\n",
    "    report = report_train + '\\n' #+ report_test + '\\n\\n'\n",
    "    if epoch % 5 == 0:\n",
    "        model_path = os.path.join(result_dir, 'model_stats', f'Model_epoch_{epoch}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    with open(os.path.join(result_dir, 'accuracy_stats', 'report_train.txt'), 'a') as f:\n",
    "        f.write(report)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
